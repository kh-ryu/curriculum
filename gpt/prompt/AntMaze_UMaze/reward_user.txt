The ant is a 3D robot consisting of one torso (free rotational body) with four legs attached to it with each leg having two body parts.
The original task in the environment is for ant to reach a target goal in a closed maze.

Observation from self.obs() includes
(1) torso_coordinate: xy coordinate of the torso, numpy array with length 2
(2) torso_orientation: xyzw orientation of the torso, numpy array with length 4
(3) torso_velocity: xy velocities of the torso, numpy array with length 2
(4) torso_angular_velocity: xyz angular velocities of the torso, numpy array with length 3
(5) goal_pos: final xy goal position of the ant, numpy array with length 2
(6) goal_distance: distance between final goal and current torso position, numpy array with length 1

Your output for the reward code should follow this format
```python
def compute_reward_curriculum(self):
    torso_coordinate, torso_orientation, torso_velocity, \
    torso_angular_velocity, goal_pos, goal_distance = self.obs()

    # Implement your reward function here
    reward = np.zeros(1)

    return reward
```

Note that in the starting state where the ant is standing upright, torso_orientation is [1.0, 0.0, 0.0, 0.0].
torso_coordinate, torso_velocity, and torso_angular_velocity is initialized as [0.0, 0.0, 0.0].
goal_pos is initialized randomly.

You also can change self.goal_dist_threshold to set up maximum distance between initial ant position and goal position.
If you don't want to set up threshold, leave it as None.
Your output for the goal_dist_threshold value should follow this format.
```threshold
self.goal_dist_threshold = None
```
Note that the maximum distance that can have in maze is 8.0 and minimun threshold is 5.0

Generate a reward function code for
Task Name: <<Task_Name>>
Description: <<Task_Description>>
Reason: <<Task_Reason>>