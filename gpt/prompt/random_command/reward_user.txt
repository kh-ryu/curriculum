The hub is a bipedal robot that can walk and run in a fashin similar to that of humans or animals.
The original task in the environment is for hub to walk or run by following random command.
Speed of the hub robot will be specified in a command consist of [linear velocity x, linear velocity y, angular velocity z, heading angle]
The command range in original task should follow this: "linear velocity x: [-2, 2] , linear velocity y: [-2, 2], heading angle is [-pi, pi]"

observation from normalized_obs() includes
base_lin_vel: Linear velocity of base in xyz direction, 2 dimension with shape (number of envs, 3)
base_ang_vel: angular velocity of base in xyz direction, 2 dimension with shape (number of envs, 3)
nonflat_base_orientation: Size of deviation of base orientation from default standing position, 1 dimension with shape (number of envs)
base_height_diff: height difference between measured base height and default stainding height, 1 dimension with shape (number of envs)
torques: Torques applied to the joint, 2 dimension with shape (number of envs, number of joints)
hip_pos_deviation: Relative position of hip joint from default standing position, 2 dimension with shape (number of envs, number of hip joints)
knee_pos_deviation: Relative position of knee joint from default standing position, 2 dimension with shape (number of envs, number of knee joints)
joint_vel: Velocity of each joint, 2 dimension with shape (number of envs, number of joints)
joint_acc: Acceleration of each joint, 2 dimension with shape (number of envs, number of joints)
is_alive: 1 if the environment is healthy, 0 if the environment is terminated due to failure, 1 dimension with shape (number of envs)

commands: Commands that robot base should follow. 
          Consist of [linear velocity x, linear velocity y, angular velocity z].
          2 dimension with shape (number ov envs, 3)

Note that default values are initialized to make the bipedal robot stand.

Your output for the environment code should follow this format
```python
def reward_curriculum(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Compute the reward for the current curriculum step."""
    command, base_lin_vel, base_ang_vel, hip_pos_deviation, knee_pos_deviation, \
    joint_vel, joint_acc, torque, \
    nonflat_base_orientation, base_height_diff = normalized_obs(env, asset_cfg)
    is_alive = (~env.termination_manager.terminated).float()

    n_envs = env.scene.num_envs

    # Implement your reward function here
    reward = torch.zeros(n_envs, device=env.device)
    return reward
```

You should re-implement reward_curriculum function to assign a proper reward function for given task. 
Note that you should try to follow command linear velocity and angular velocity z.
Especially, third component of base angular velocity should accurately follow angular velocity z in command.


Your output for the command value should follow this format.
You should change the range of lin_vel_x, lin_vel_y, and heading for your specific task.
You cannot change the range of ang_vel_z.
```command        
ranges=mdp.UniformVelocityCommandCfg.Ranges(lin_vel_x=(-2.0, 2.0), lin_vel_y=(-2.0, 2.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)),
```

Generate a reward function code and command for
Task Name: <<Task_Name>>
Description: <<Task_Description>>
Reason: <<Task_Reason>>
