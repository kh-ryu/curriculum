{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 11:38:38.961463: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 11:38:45.394030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from evaluation.evalcallback_feedback import CurriculumEvalCallback\n",
    "\n",
    "from gpt.utils import file_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_curriculum_training_log(logger_path):\n",
    "    training_log = np.load(logger_path + \"/evaluations.npz\", allow_pickle=True)\n",
    "\n",
    "    reward_dict = training_log[\"results_dict\"]\n",
    "    success = training_log[\"successes\"].mean(axis=1)\n",
    "\n",
    "    averaged_dicts = []\n",
    "\n",
    "    for row in reward_dict:\n",
    "        sum_dict = {}\n",
    "        for col in row:\n",
    "            for key in col:\n",
    "                sum_dict[key] = sum_dict.get(key, 0) + col[key]\n",
    "\n",
    "        avg_dict = {key: value/len(row) for key, value in sum_dict.items()}\n",
    "        averaged_dicts.append(avg_dict)\n",
    "\n",
    "    reward_df = pd.DataFrame(averaged_dicts)\n",
    "\n",
    "    return reward_df, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_log(logger_path):\n",
    "    training_log = np.load(logger_path + \"/evaluations.npz\", allow_pickle=True)\n",
    "    \n",
    "    reward_main = training_log[\"results\"].mean(axis=1)\n",
    "    try:\n",
    "        success = training_log[\"successes\"].mean(axis=1)\n",
    "    except:\n",
    "        success = None\n",
    "    \n",
    "    return reward_main, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_curriculum(logger_path):\n",
    "    # extract curriculum and return list of dictionaries with task details\n",
    "    curriculum_txt = file_to_string(logger_path + \"curriculum.md\")\n",
    "    # Split the string into individual task sections\n",
    "    task_sections = re.split(r'\\n\\n(?=Task)', curriculum_txt)\n",
    "\n",
    "    # Function to extract details from each task section\n",
    "    def extract_task_details(task_section):\n",
    "\n",
    "        details = {}\n",
    "        lines = task_section.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('Task'):\n",
    "                details['Task'] = line.split(' ')[1]\n",
    "            elif line.startswith('Name:'):\n",
    "                details['Name'] = line.split(': ')[1]\n",
    "            elif line.startswith('Description:'):\n",
    "                details['Description'] = line.split(': ')[1]\n",
    "            elif line.startswith('Reason:'):\n",
    "                details['Reason'] = ': '.join(line.split(': ')[1:])\n",
    "        return details\n",
    "\n",
    "    # Extract details for all tasks\n",
    "    curriculum_info = [extract_task_details(section) for section in task_sections]\n",
    "    curriculum_length = len(curriculum_info)\n",
    "    \n",
    "    return curriculum_info, curriculum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_agent(logger_path, curriculum_info, curriculum_length):\n",
    "    task_list = []\n",
    "    best_agent_list = []\n",
    "    for idx in range(curriculum_length):\n",
    "        curriculum_name = curriculum_info[idx]['Name']\n",
    "        task_list.append(curriculum_name)\n",
    "        try:\n",
    "            decision = file_to_string(logger_path + curriculum_name + '.md')\n",
    "            decision = decision.split('\\n')[0]\n",
    "            numbers = re.findall(r'\\d+', decision)\n",
    "        except:\n",
    "            numbers = [0]\n",
    "        if numbers:\n",
    "            best_agent_list.append(int(numbers[0]))\n",
    "        else:\n",
    "            print(f\"No number found in the decision {idx}\")\n",
    "            best_agent_list.append(0)\n",
    "            \n",
    "    return task_list, best_agent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(logger_path, curriculum_exp_num, her_exp_num, sac_exp_num, scratch_exp_num):\n",
    "    window_size = 10\n",
    "    # Load the training logs for curriculum experiment\n",
    "    curriculum_success_list = []\n",
    "    for i in range(curriculum_exp_num):\n",
    "        print(\"Loading curriculum experiment: \", i)\n",
    "        curriculum_logger_path = logger_path + \"curriculum_\" + str(i) + \"/\"\n",
    "        curriculum_info, curriculum_length = extract_curriculum(curriculum_logger_path)\n",
    "        task_list, best_sample_idx = extract_best_agent(curriculum_logger_path, curriculum_info, curriculum_length)\n",
    "\n",
    "        reward_main, reward_task, success, task_list = [], [], [], []\n",
    "        for idx, task in enumerate(task_list):\n",
    "            path = curriculum_logger_path + task + f\"/sample_{best_sample_idx[idx]}\"\n",
    "            \n",
    "            reward_df, success = load_curriculum_training_log(path)\n",
    "\n",
    "            print(\"Reward arguments in Task: \", task[\"Name\"])\n",
    "            for key in reward_df.keys():\n",
    "                print(key)\n",
    "\n",
    "            reward_main.append(reward_df[\"main\"])\n",
    "            reward_task.append(reward_df[\"task\"])\n",
    "            success_list.append(success)\n",
    "            task_length.append(len(reward_df[\"main\"]))\n",
    "\n",
    "        reward_main = np.concatenate(reward_main, axis=0)\n",
    "        reward_task = np.concatenate(reward_task, axis=0)\n",
    "        success = np.concatenate(success_list, axis=0)\n",
    "\n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        curriculum_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Load the training logs for HER experiment\n",
    "    her_success_list = []\n",
    "    for i in range(her_exp_num):\n",
    "        print(\"Loading HER experiment: \", i)\n",
    "        her_logger_path = logger_path + \"her_\" + str(i) + \"/\"\n",
    "        reward_main, success = load_training_log(her_logger_path)\n",
    "        \n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        her_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Load the training logs for SAC experiment\n",
    "    sac_success_list = []\n",
    "    for i in range(sac_exp_num):\n",
    "        print(\"Loading SAC experiment: \", i)\n",
    "        sac_logger_path = logger_path + \"sac_\" + str(i) + \"/\"\n",
    "        reward_main, success = load_training_log(sac_logger_path)\n",
    "        \n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        sac_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Load the training logs for Scratch experiment\n",
    "    scratch_success_list = []\n",
    "    for i in range(scratch_exp_num):\n",
    "        print(\"Loading Scratch experiment: \", i)\n",
    "        scratch_logger_path = logger_path + \"scratch_\" + str(i) + \"/\"\n",
    "        reward_main, success = load_training_log(scratch_logger_path)\n",
    "        \n",
    "        # compute the moving average of success rate\n",
    "        success_moving_avg = pd.Series(success).rolling(window_size).mean().values\n",
    "        scratch_success_list.append(success_moving_avg)\n",
    "\n",
    "    # Plot the success rate in same figure\n",
    "    curriculum_success_list = np.array(curriculum_success_list)\n",
    "    her_success_list = np.array(her_success_list)\n",
    "    sac_success_list = np.array(sac_success_list)\n",
    "    scratch_success_list = np.array(scratch_success_list)\n",
    "\n",
    "    curriculum_success_avg = np.mean(curriculum_success_list, axis=0)\n",
    "    her_success_avg = np.mean(her_success_list, axis=0)\n",
    "    sac_success_avg = np.mean(sac_success_list, axis=0)\n",
    "    scratch_success_avg = np.mean(scratch_success_list, axis=0)\n",
    "\n",
    "    curriculum_success_std = np.std(curriculum_success_list, axis=0)\n",
    "    her_success_std = np.std(her_success_list, axis=0)\n",
    "    sac_success_std = np.std(sac_success_list, axis=0)\n",
    "    scratch_success_std = np.std(scratch_success_list, axis=0)\n",
    "\n",
    "    # Assign color to each experiments\n",
    "    curriculum_color = 'blue'\n",
    "    her_color = 'green'\n",
    "    sac_color = 'red'\n",
    "    scratch_color = 'orange'\n",
    "\n",
    "    # Plot the success rate\n",
    "    plt.figure()\n",
    "    plt.plot(curriculum_success_avg, label='Curriculum', color=curriculum_color)\n",
    "    plt.fill_between(np.arange(len(curriculum_success_avg)), curriculum_success_avg - curriculum_success_std, curriculum_success_avg + curriculum_success_std, color=curriculum_color, alpha=0.2)\n",
    "    plt.plot(her_success_avg, label='HER', color=her_color)\n",
    "    plt.fill_between(np.arange(len(her_success_avg)), her_success_avg - her_success_std, her_success_avg + her_success_std, color=her_color, alpha=0.2)\n",
    "    plt.plot(sac_success_avg, label='SAC', color=sac_color)\n",
    "    plt.fill_between(np.arange(len(sac_success_avg)), sac_success_avg - sac_success_std, sac_success_avg + sac_success_std, color=sac_color, alpha=0.2)\n",
    "    plt.plot(scratch_success_avg, label='Scratch', color=scratch_color)\n",
    "    plt.fill_between(np.arange(len(scratch_success_avg)), scratch_success_avg - scratch_success_std, scratch_success_avg + scratch_success_std, color=scratch_color, alpha=0.2)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.legend()\n",
    "    plt.title('Success Rate')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the figure to logger path\n",
    "    plt.savefig(logger_path + \"success_rate.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_path = \"logs/AntMaze_UMaze/\"\n",
    "curriculum_exp_num = 1\n",
    "her_exp_num = 1\n",
    "sac_exp_num = 1\n",
    "scratch_exp_num = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 (TF-2.12.0)",
   "language": "python",
   "name": "python3.10.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
