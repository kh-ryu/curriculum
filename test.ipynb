{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gpt.curriculum_api import CurriculumAPI\n",
    "from gpt.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = \"./gpt/prompt/\"\n",
    "env_name = \"Fetch_Push\"\n",
    "\n",
    "API = CurriculumAPI(env_name, prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1\n",
      "Name: Basic Movement\n",
      "Description: maximize torso_velocity\n",
      "Reason: This task aims to teach the ant basic movement skills. Increasing torso velocity will ensure it can move from its initial position, which is crucial for later tasks requiring navigation towards a goal.\n",
      "\n",
      "Task 2\n",
      "Name: Orientation Maintenance\n",
      "Description: maintain torso_orientation as a value of [1.0, 0.0, 0.0, 0.0]\n",
      "Reason: Maintaining a stable orientation is crucial for effective movement and navigation through the maze. This task ensures the ant learns how to stabilize itself, which is essential for performing precise movements towards the goal.\n",
      "\n",
      "Task 3\n",
      "Name: Angular Velocity Control\n",
      "Description: minimize torso_angular_velocity\n",
      "Reason: This task teaches the ant to control and minimize its angular velocity. A lower angular velocity is important for precision and stability, especially in a maze environment where sudden changes in direction can lead to failure.\n",
      "\n",
      "Task 4\n",
      "Name: Navigation to Intermediate Goals\n",
      "Description: minimize goal_distance with intermediary goal_pos\n",
      "Reason: Before tackling the final goal in a complex maze, learning to navigate to intermediate goals is useful. This task will teach the ant to effectively adjust its path and strategies in real time to reach specific locations, preparing it for the unpredictability of the final maze layout.\n",
      "\n",
      "Task 5\n",
      "Name: Final Goal Navigation\n",
      "Description: minimize goal_distance as 0.45\n",
      "Reason: This task combines all skills learned in prior tasks to achieve the ultimate goal of reaching the final target in the maze. It emphasizes precise control, stability, and effective navigation, simulating the conditions of the original task in a complex environment.\n"
     ]
    }
   ],
   "source": [
    "tasks_details = API.generate_curriculum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"```python\n",
      "def compute_reward_curriculum(self):\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    \n",
      "    # Reward components\n",
      "    velocity_reward_weight = 1.0\n",
      "    \n",
      "    # Reward calculations\n",
      "    velocity_reward = np.tanh(np.linalg.norm(torso_vel)) # Maximize torso velocity\n",
      "    \n",
      "    # Total reward calculation\n",
      "    total_reward = velocity_reward_weight * velocity_reward\n",
      "    \n",
      "    # Reward dictionary\n",
      "    reward_dict = {\n",
      "        'velocity_reward': velocity_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_dict\n",
      "```\"\n",
      "Extracted Code Block:\n",
      " def compute_reward_curriculum(self):\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    \n",
      "    # Reward components\n",
      "    velocity_reward_weight = 1.0\n",
      "    \n",
      "    # Reward calculations\n",
      "    velocity_reward = np.tanh(np.linalg.norm(torso_vel)) # Maximize torso velocity\n",
      "    \n",
      "    # Total reward calculation\n",
      "    total_reward = velocity_reward_weight * velocity_reward\n",
      "    \n",
      "    # Reward dictionary\n",
      "    reward_dict = {\n",
      "        'velocity_reward': velocity_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_dict\n",
      "Updated environment code saved to ./environments/Curriculum/envs/ant_maze_v0.py\n"
     ]
    }
   ],
   "source": [
    "env_code_path = \"./environments/Curriculum/envs/Fetch_Push.py\"\n",
    "task = tasks_details[0]\n",
    "\n",
    "code_1 = \"\"\"\n",
    "def compute_reward_curriculum(self):\n",
    "    ant_obs = self.get_ant_obs()\n",
    "    goal_distance = self.goal_distance(ant_obs)\n",
    "    velocity_weight = 0.1\n",
    "    goal_distance_weight = 1.0\n",
    "\n",
    "    # Reward for moving towards the goal\n",
    "    reward_for_goal_distance = -np.tanh(goal_distance) * goal_distance_weight\n",
    "\n",
    "    # Reward for maintaining a certain velocity (encourage movement)\n",
    "    torso_velocity = self.torso_velocity(ant_obs)\n",
    "    desired_velocity = np.array([1.0, 0.0]) # Example desired velocity towards the goal\n",
    "    velocity_difference = np.linalg.norm(torso_velocity - desired_velocity)\n",
    "    reward_for_velocity = -np.tanh(velocity_difference) * velocity_weight\n",
    "\n",
    "    # Total reward\n",
    "    total_reward = reward_for_goal_distance + reward_for_velocity\n",
    "\n",
    "    reward_dict = {\n",
    "        'goal_distance': reward_for_goal_distance,\n",
    "        'velocity': reward_for_velocity\n",
    "    }\n",
    "\n",
    "    return total_reward, reward_dict\n",
    "\"\"\"\n",
    "previous_reward_code = [code_1]\n",
    "\n",
    "reward_code = API.update_env_code(env_code_path, task, previous_reward_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 19:14:08.606370: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-25 19:14:08.606393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-25 19:14:08.606406: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-25 19:14:08.610315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/kh-ryu/anaconda3/envs/curriculum/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Curriculum/AntMaze_UMaze-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "import Curriculum\n",
    "from utils.train_utils import *\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env_id = \"Curriculum/Fetch_Push-v0\"\n",
    "num_cpu = 4\n",
    "env = make_vec_env(\"Curriculum/Fetch_Push-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8548721]\n",
      "[-0.8569425]\n",
      "[-0.90360034]\n",
      "[-0.67222303]\n",
      "[-0.7980592]\n",
      "[-0.48293692]\n",
      "[-0.37922052]\n",
      "[-0.32729062]\n",
      "[-0.8477994]\n",
      "[-0.52121484]\n",
      "[-0.27145526]\n",
      "[-0.49351692]\n",
      "[-0.43549073]\n",
      "[-0.4660426]\n",
      "[-0.33359784]\n",
      "[-0.2809185]\n",
      "[-0.22043951]\n",
      "[-0.18211484]\n",
      "[-0.34708044]\n",
      "[-0.18931317]\n",
      "[-0.28505123]\n",
      "[-0.16576925]\n",
      "[-0.27528203]\n",
      "[-0.34637013]\n",
      "[-0.49647218]\n",
      "[-0.72735095]\n",
      "[-0.4839832]\n",
      "[-0.4058162]\n",
      "[-0.45727134]\n",
      "[-0.45051712]\n",
      "[-0.69855565]\n",
      "[-0.91694945]\n",
      "[-0.3660304]\n",
      "[-0.4896844]\n",
      "[-0.33393237]\n",
      "[-0.7600975]\n",
      "[-0.43458778]\n",
      "[-0.41114312]\n",
      "[-0.49821347]\n",
      "[-0.3092446]\n",
      "[-0.16893642]\n",
      "[-0.16343866]\n",
      "[-0.18539545]\n",
      "[-0.21579468]\n",
      "[-0.42603213]\n",
      "[-0.54612494]\n",
      "[-0.4008043]\n",
      "[-0.38511106]\n",
      "[-0.6391636]\n",
      "[-0.67948455]\n",
      "[-0.49913564]\n",
      "[-0.87358236]\n",
      "[-0.3018769]\n",
      "[-0.5179596]\n",
      "[-0.4549645]\n",
      "[-0.26950166]\n",
      "[-0.42400232]\n",
      "[-0.15804742]\n",
      "[-0.16362563]\n",
      "[-0.27348998]\n",
      "[-0.17178498]\n",
      "[-0.15780343]\n",
      "[-0.16048759]\n",
      "[-0.18210576]\n",
      "[-0.20815045]\n",
      "[-0.1710233]\n",
      "[-0.1437928]\n",
      "[-0.13402286]\n",
      "[-0.3011705]\n",
      "[-0.17997427]\n",
      "[-0.1530135]\n",
      "[-0.18660095]\n",
      "[-0.13074161]\n",
      "[-0.11431146]\n",
      "[-0.13340102]\n",
      "[-0.14242505]\n",
      "[-0.15036611]\n",
      "[-0.14437708]\n",
      "[-0.12047389]\n",
      "[-0.11548329]\n",
      "[-0.276788]\n",
      "[-0.28981483]\n",
      "[-0.23431095]\n",
      "[-0.24010608]\n",
      "[-0.19902712]\n",
      "[-0.3852575]\n",
      "[-0.5999652]\n",
      "[-0.6090082]\n",
      "[-0.37625527]\n",
      "[-0.4909247]\n",
      "[-0.7211452]\n",
      "[-0.96551466]\n",
      "[-0.5151736]\n",
      "[-0.63158447]\n",
      "[-0.4024191]\n",
      "[-0.3108596]\n",
      "[-0.48529866]\n",
      "[-0.3932971]\n",
      "[-0.23893236]\n",
      "[-0.27617782]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action = [env.action_space.sample(), env.action_space.sample(), env.action_space.sample(), env.action_space.sample()]\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curriculum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
