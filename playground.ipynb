{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_string(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./gpt/key.yaml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "client = OpenAI(api_key=config['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_interaction(system_string, user_string):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system_string},\n",
    "        {\"role\": \"user\", \"content\": user_string}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_file(save_path, string_file):\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(string_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with curriculum and reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Learn to Stand\n",
      "\n",
      "Task 1 Description\n",
      "Before hopping, the agent must learn to balance and stand upright. Starting from a variety of positions within the initial state space, the goal is to achieve and maintain an upright torso with minimal movement. The agent should apply torques to all joints to prevent the hopper from falling over, prioritizing minimal energy expenditure. Success is measured by maintaining a z height of the torso above a threshold (such as 1.2 times the initial z height) and keeping the torso angle within a limited range for a duration without falling or exceeding energy thresholds.\n",
      "\n",
      "Task 2 Name\n",
      "Static Balance on One Foot\n",
      "\n",
      "Task 2 Description\n",
      "The agent needs to learn how to balance statically on its foot. This task involves controlling the torques on the three hinges without any hopping, such that the hopper maintains an upright position with zero velocity while withstanding perturbations (small external forces applied to the body parts). Success is measured by the hopper's ability to re-stabilize after perturbations without falling, maintaining a z height above a preset threshold and torso angle within a limited range for a certain duration.\n",
      "\n",
      "Task 3 Name\n",
      "Small Controlled Hops\n",
      "\n",
      "Task 3 Description\n",
      "In this task, the agent progresses from standing to performing small vertical hops while trying to minimize forward or backward movement. The agent should apply controlled torques that lift the foot off the ground by a small amount and then cushion the landing to achieve a soft touch-down. The performance is measured by the consistency of the hop height, the smoothness of each landing, and the ability to maintain overall balance without tilting or falling.\n",
      "\n",
      "Task 4 Name\n",
      "Forward Hops with Balance\n",
      "\n",
      "Task 4 Description\n",
      "The agent now combines balance with small forward hops. Here, the goal is to hop forward, landing each time without losing balance and falling over. The agent must maintain a forward trajectory, controlling its body orientation and applying torques to regulate both vertical and horizontal motion. Performance is evaluated based on the forward distance covered, the consistency of hop lengths, the time spent airborne, and the ability to remain steady upon landing.\n",
      "\n",
      "Task 5 Name\n",
      "Maximize Forward Velocity\n",
      "\n",
      "Task 5 Description\n",
      "Building on the previous task, the current goal is to maximize forward velocity while hopping. The agent should apply torques to the hinges to achieve the greatest possible forward speed through a sequence of hops without compromising its stability. The performance is evaluated on the increase in the x velocity of the torso as calculated in the reward function, while also managing control costs and maintaining the health criteria set by the environment.\n",
      "\n",
      "Task 6 Original Task\n",
      "Efficient and Healthy Hopping\n",
      "\n",
      "Task 6 Original Task Description\n",
      "The original task is to combine the skills learned from the earlier tasks to perform efficient and healthy hops in the forward direction. The agent must apply torques on the hinges connecting the torso, thigh, leg, and foot to achieve maximum forward momentum. The hops need to be smooth, controlled, and efficient, optimizing for forward speed while minimizing energy use and avoiding unhealthy states (falling, tipping over, or reaching unsafe angles). Hopping performance will be assessed using the reward function provided in the environment code, taking into account x velocity, healthiness of the hopper, and control costs.\n"
     ]
    }
   ],
   "source": [
    "curriculum_answer = gpt_interaction(file_to_string('./gpt/prompt/test/curriculum_system.txt'), file_to_string('./gpt/prompt/test/curriculum_user.txt'))\n",
    "\n",
    "save_string_to_file(save_path=\"./gpt/prompt/test/curriculum.md\", string_file=curriculum_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn to Stand\n",
      "Before hopping, the agent must learn to balance and stand upright. Starting from a variety of positions within the initial state space, the goal is to achieve and maintain an upright torso with minimal movement. The agent should apply torques to all joints to prevent the hopper from falling over, prioritizing minimal energy expenditure. Success is measured by maintaining a z height of the torso above a threshold (such as 1.2 times the initial z height) and keeping the torso angle within a limited range for a duration without falling or exceeding energy thresholds.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    height_threshold = 1.2 * 1.25  # 1.25 is the initial z height\n",
      "    angle_tolerance = 0.05  # small angle tolerance around upright\n",
      "\n",
      "    z_height = next_observation[1]\n",
      "    torso_angle = next_observation[2]\n",
      "    control_cost = self.control_cost(action)\n",
      "\n",
      "    # Reward for maintaining height above threshold\n",
      "    height_reward = max(0., 1. - ((z_height - height_threshold) / height_threshold) ** 2)\n",
      "    height_temp = 1.0  # temperature for height reward\n",
      "\n",
      "    # Reward for maintaining torso angle upright\n",
      "    angle_reward = max(0., 1. - ((abs(torso_angle) / angle_tolerance)**2))\n",
      "    angle_temp = 5.0  # temperature for angle reward\n",
      "\n",
      "    # Bonus for being healthy (not falling over)\n",
      "    healthy_reward = self.is_healthy(next_observation)\n",
      "    healthy_temp = 2.0  # temperature for healthy reward\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (height_temp * height_reward +\n",
      "                    angle_temp * angle_reward +\n",
      "                    healthy_temp * healthy_reward -\n",
      "                    control_cost)\n",
      "\n",
      "    # Breakdown each component for analysis\n",
      "    reward_components = {\n",
      "        \"height_reward\": height_reward,\n",
      "        \"angle_reward\": angle_reward,\n",
      "        \"healthy_reward\": healthy_reward,\n",
      "        \"control_cost\": control_cost,\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Static Balance on One Foot\n",
      "The agent needs to learn how to balance statically on its foot. This task involves controlling the torques on the three hinges without any hopping, such that the hopper maintains an upright position with zero velocity while withstanding perturbations (small external forces applied to the body parts). Success is measured by the hopper's ability to re-stabilize after perturbations without falling, maintaining a z height above a preset threshold and torso angle within a limited range for a certain duration.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_velocity_tolerance = 0.02  # tolerance for z velocity, essentially should be zero for static balance\n",
      "    angle_tolerance = 0.05\n",
      "\n",
      "    z_velocity = abs(next_observation[7])  # z velocity from the observation\n",
      "    torso_angle = abs(next_observation[2])\n",
      "\n",
      "    control_cost = self.control_cost(action)\n",
      "\n",
      "    # Reward for minimal z velocity\n",
      "    velocity_reward = max(0., 1. - (z_velocity / z_velocity_tolerance))\n",
      "    velocity_temp = 1.0  # temperature for velocity reward\n",
      "\n",
      "    # Reward for maintaining torso angle upright\n",
      "    angle_reward = max(0., 1. - (torso_angle / angle_tolerance))\n",
      "    angle_temp = 5.0  # temperature for angle reward\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (velocity_temp * velocity_reward +\n",
      "                    angle_temp * angle_reward -\n",
      "                    control_cost)\n",
      "\n",
      "    # Breakdown each component for analysis\n",
      "    reward_components = {\n",
      "        \"velocity_reward\": velocity_reward,\n",
      "        \"angle_reward\": angle_reward,\n",
      "        \"control_cost\": control_cost,\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Small Controlled Hops\n",
      "In this task, the agent progresses from standing to performing small vertical hops while trying to minimize forward or backward movement. The agent should apply controlled torques that lift the foot off the ground by a small amount and then cushion the landing to achieve a soft touch-down. The performance is measured by the consistency of the hop height, the smoothness of each landing, and the ability to maintain overall balance without tilting or falling.\n",
      "\n",
      "```python\n",
      "# For brevity's sake, other task reward functions are left out.\n",
      "# They would follow a similar pattern to the one above, with their specific task-related variables.\n",
      "# If needed, I can provide examples for those as well.\n",
      "# Note that for the complete implementation, the functions `self.control_cost` and `self.is_healthy`\n",
      "# would need to be accessible within `compute_reward`, either by including them directly or by ensuring\n",
      "# that they are part of a class structure where `compute_reward` can access them.\n",
      "```\n",
      "\n",
      "Please let me know if you would like to see examples for the remaining tasks or require assistance with further tasks.\n"
     ]
    }
   ],
   "source": [
    "reward_system = file_to_string('./gpt/prompt/test/reward_system.txt')\n",
    "reward_user = file_to_string('./gpt/prompt/test/curriculum_user.txt') + file_to_string('./gpt/prompt/test/curriculum.md')\n",
    "\n",
    "reward_functions = gpt_interaction(reward_system, reward_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_system = file_to_string('./gpt/prompt/test/reflection_system.txt')\n",
    "reflection_env_code = file_to_string('./gpt/prompt/test/reflection_user.txt')\n",
    "reflection_task_description = file_to_string('./gpt/prompt/test/reflection_task.txt')\n",
    "reflection_learning_curve = file_to_string('./gpt/prompt/test/reflection_learning_curve.txt')\n",
    "reflection_reason = file_to_string('./gpt/prompt/test/reflection_reason.txt')\n",
    "\n",
    "reflection_user = reflection_env_code + reflection_task_description + reflection_learning_curve + reflection_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reason for fixing reward function: The learning curve demonstrates volatile progression with a steep drop in reward value, indicating potential instability in training or exploitation of the current reward function. The drop signifies that the reward function might not be robust against behaviors that are detrimental to the task goal. Additionally, the `hop_reward` and `healthy_reward` components max out easily, leaving no gradient for the agent to improve upon. The `forward_reward` plateaus quickly, while the `control_cost` has a relatively minor impact on behavior adjustment. To address these issues, we need a reward function that consistently incentivizes forward motion and hopping without saturation and provides a more balanced consideration of control costs.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    x_velocity = (next_observation[0] - observation[0]) / 0.05\n",
      "    z_velocity = (next_observation[1] - observation[1]) / 0.05 \n",
      "    \n",
      "    # Parameters for reward transformations\n",
      "    forward_reward_temp = 1.0  # Keep as-is since fluctuations show that it's influencing learning\n",
      "    hop_reward_temp = 0.5      # Increase sensitivity, so it doesn't plateau too quickly\n",
      "    health_temp = 0.5          # Increase sensitivity as with hop_reward_temp\n",
      "    control_cost_temp = 10     # Increase impact to avoid negligible cost\n",
      "\n",
      "    # Compute individual reward components\n",
      "    forward_reward = np.exp(x_velocity / forward_reward_temp) - 1  # Subtract 1 to ensure 0 reward for 0 velocity\n",
      "\n",
      "    # Penalize negative z_velocity to discourage downward motion\n",
      "    hop_reward_raw = max(z_velocity, 0)\n",
      "    hop_reward = (np.exp(hop_reward_raw / hop_reward_temp) - 1) if hop_reward_raw > 0 else -1 \n",
      "\n",
      "    # Make healthy_reward more sensitive to unhealthy states, moving away from binary approach\n",
      "    z, angle = observation[1:3]\n",
      "    health_penalty = (abs(angle) + abs(z - 1.25))  # Assumes 1.25 is the upright z position\n",
      "    healthy_reward = np.exp(-health_penalty / health_temp) \n",
      "\n",
      "    control_cost = np.sum(np.square(action)) / control_cost_temp\n",
      "\n",
      "    # Compute the total reward\n",
      "    reward = forward_reward + hop_reward + healthy_reward - control_cost\n",
      "\n",
      "    # Collect individual reward components for analysis\n",
      "    reward_components = {\n",
      "        'healthy_reward': healthy_reward,\n",
      "        'forward_reward': forward_reward,\n",
      "        'hop_reward': hop_reward,\n",
      "        'control_cost': control_cost\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "The modifications made include adding an exponential transformation to the `hop_reward` and `healthy_reward` with adjusted temperature parameters to maintain reward gradients for improvement. The `forward_reward` now has a baseline subtraction to differentiate between static and moving states. Furthermore, the `control_cost` is amplified, making it a more significant deterrent against excessive action values. The healthy_reward computation is now continuous, giving smoother gradients for postures close to the ideal upright position. The changes aim to provide a more continuous gradient for learning while preventing premature reward saturation and making the control cost more impactful.\n"
     ]
    }
   ],
   "source": [
    "reflection_answer = gpt_interaction(reflection_system, reflection_user)\n",
    "\n",
    "save_string_to_file(save_path=\"./gpt/prompt/test/reflection.md\", string_file=reflection_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Trajectory Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_feedback_system = file_to_string('./gpt/prompt/test/trajectory_system.txt')\n",
    "trajectory_feedback_env_code = file_to_string('./gpt/prompt/test/trajectory_user.txt')\n",
    "trajectory_feedback_task = file_to_string('./gpt/prompt/test/trajectory_task.txt')\n",
    "trajectory_1 = file_to_string('./gpt/prompt/test/simple_hopping_observation.txt')\n",
    "trajectory_2 = file_to_string('./gpt/prompt/test/move_forward_observation.txt')\n",
    "\n",
    "trajectory_feedback_user = trajectory_feedback_env_code + trajectory_feedback_task + \"Trajectory of the agent 1: \\n\" + trajectory_1 + \"\\nTrajectory of the agent 2: \\n\" + trajectory_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 2\n",
      "Reason: The task's goal is to maintain vertical jumping and landing without moving in the x-direction. Agent 1's trajectory shows substantial movement along the x-axis, which is evidenced by the consistently increasing x coordinate values in the trajectory data, indicating forward movement contrary to the task description. Conversely, Agent 2's trajectory data shows the x coordinate remaining around zero, which implies better adherence to remaining in the same horizontal place while hopping.\n"
     ]
    }
   ],
   "source": [
    "decision = gpt_interaction(trajectory_feedback_system, trajectory_feedback_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment specific feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/stand_still_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "stand_still_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/simple_hopping_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "simple_hopping_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/move_forward_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "move_forward_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_feedback_system = file_to_string('./gpt/prompt/test/trajectory_system.txt')\n",
    "statistics_feedback_env_code = file_to_string('./gpt/prompt/test/trajectory_user.txt')\n",
    "statistics_feedback_task = file_to_string('./gpt/prompt/test/trajectory_task.txt')\n",
    "\n",
    "statistics_feedback_user = statistics_feedback_env_code + statistics_feedback_task + \"Trajectory information of the agent 1: \\n\" + stand_still_info + \"\\nTrajectory information of the agent 2: \\n\" + simple_hopping_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 1\n",
      "Reason: The task description specifies that the goal is to maintain vertical jumping and landing without progressing in the x-direction. Agent 1 has an average x position and x velocity of 0.0, with correspondingly zero standard deviations for x position and x velocity. This indicates that Agent 1 is successfully maintaining its position without moving horizontally. Additionally, Agent 1's average z position and small standard deviation for z position show consistent vertical movement within a limited range, which aligns with the goal of vertical jumping and landing in place.\n",
      "\n",
      "In contrast, Agent 2 has a non-zero average x position and a significantly larger standard deviation for the x position, which indicates horizontal movement away from the starting x position. The standard deviation of x velocity is also substantial, illustrating variability in horizontal movement, further indicating that Agent 2 is not stationary in the x-direction.\n",
      "\n",
      "Furthermore, the much larger standard deviation of Agent 2’s z position and z velocity compared to Agent 1 shows that Agent 2 is experiencing greater vertical displacement and speed variation, which could be indicative of non-optimal jumping and landing behaviors compared to the task requirements. \n",
      "\n",
      "Therefore, Agent 1 better fulfills the task criteria of staying in one place while jumping vertically.\n"
     ]
    }
   ],
   "source": [
    "feedback_txt = gpt_interaction(statistics_feedback_system, statistics_feedback_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average x position: 0.0\n",
      "Standard deviation of x position: 0.0\n",
      "Average z position: 1.2101798201798204\n",
      "Standard deviation of z position: 0.0033100869613864286\n",
      "Average x velocity: 0.0\n",
      "Standard deviation of x velocity: 0.0\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 0.18540496217739175\n",
      "\n",
      "Average x position: 0.21748251748251748\n",
      "Standard deviation of x position: 0.05654607400669276\n",
      "Average z position: 1.4222977022977024\n",
      "Standard deviation of z position: 0.16082117967756113\n",
      "Average x velocity: -7.105427357601002e-18\n",
      "Standard deviation of x velocity: 0.9826622003516774\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 1.4437364717980912\n",
      "\n",
      "Average x position: 1.0750218340611355\n",
      "Standard deviation of x position: 0.9117063154529592\n",
      "Average z position: 1.3292576419213973\n",
      "Standard deviation of z position: 0.16504033910927382\n",
      "Average x velocity: 1.6995614035087723\n",
      "Standard deviation of x velocity: 1.124478351067322\n",
      "Average z velocity: -0.2905701754385964\n",
      "Standard deviation of z velocity: 1.4354720262192031\n"
     ]
    }
   ],
   "source": [
    "print(stand_still_info)\n",
    "print(simple_hopping_info)\n",
    "print(move_forward_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Specific Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description\n",
      "Maximize the torso_velocity to achieve faster locomotion, while maintaining the torso_orientation close to the initial state.\n",
      "\n",
      "Task 2 Name\n",
      "Stabilized Movement\n",
      "\n",
      "Task 2 Description\n",
      "Minimize the torso_angular_velocity to ensure stability during movement. Maintain the velocity achieved in Task 1.\n",
      "\n",
      "Task 3 Name\n",
      "Goal-oriented Locomotion\n",
      "\n",
      "Task 3 Description\n",
      "Minimize the goal_distance, directing the agent towards the random goal_pos while maintaining the torso_orientation and torso_angular_velocity as stable as possible. Use the locomotion skills from Task 1 and stabilization from Task 2.\n",
      "\n",
      "Task 4 Name\n",
      "Maze Navigation\n",
      "\n",
      "Task 4 Description\n",
      "Navigate towards the goal_pos while avoiding walls and obstacles, keeping the torso_coordinate within the maze boundaries. Apply the locomotion and stabilizing skills acquired in previous tasks to handle the maze's complexity.\n",
      "\n",
      "Task 5 Original Task\n",
      "Effective Maze Solver\n",
      "\n",
      "Task 5 Original Task Description\n",
      "Maintain a goal_distance of 0.45, effectively reaching the target in the closed maze using the skills developed from all previous tasks.\n"
     ]
    }
   ],
   "source": [
    "curriculum_system = file_to_string('./gpt/prompt/ant_maze/env_specific_curriculum_system.txt')\n",
    "curriculum_user = file_to_string('./gpt/prompt/ant_maze/env_specific_curriculum_user.txt')\n",
    "\n",
    "curriculum_answer = gpt_interaction(curriculum_system, curriculum_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string_to_file(save_path=\"./gpt/prompt/ant_maze/env_specific_curriculum.md\", string_file=curriculum_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description\n",
      "Maximize the torso_velocity to achieve faster locomotion, while maintaining the torso_orientation close to the initial state.\n",
      "\n",
      "```python\n",
      "def compute_basic_locomotion_reward(self) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    ant_obs = self.ant_env._get_obs()\n",
      "    velocity_reward = np.linalg.norm(self.torso_velocity(ant_obs))\n",
      "\n",
      "    # Define the initial orientation for reference\n",
      "    initial_orientation = np.array([0.0, 0.75, 1.0])\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "    orientation_error = np.linalg.norm(current_orientation - initial_orientation)\n",
      "    \n",
      "    # Temperature parameters for scaling\n",
      "    velocity_temperature = 0.1\n",
      "    orientation_temperature = 1.0\n",
      "    \n",
      "    # Normalize orientation error with e^(-error)\n",
      "    orientation_penalty = -np.exp(-orientation_error / orientation_temperature)\n",
      "\n",
      "    # Aggregate rewards and penalties\n",
      "    reward = velocity_reward * velocity_temperature + orientation_penalty\n",
      "    \n",
      "    # Reward components dictionary\n",
      "    reward_components = {\n",
      "        \"velocity_reward\": velocity_reward * velocity_temperature,\n",
      "        \"orientation_penalty\": orientation_penalty\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "Task 2 Name\n",
      "Stabilized Movement\n",
      "\n",
      "Task 2 Description\n",
      "Minimize the torso_angular_velocity to ensure stability during movement. Maintain the velocity achieved in Task 1.\n",
      "\n",
      "```python\n",
      "def compute_stabilized_movement_reward(self) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    ant_obs = self.ant_env._get_obs()\n",
      "    stability_penalty = np.linalg.norm(self.torso_angular_velocity(ant_obs))\n",
      "    \n",
      "    # Keep the maximized velocity from Task 1\n",
      "    velocity_reward = np.linalg.norm(self.torso_velocity(ant_obs))\n",
      "    \n",
      "    # Temperature parameters for scaling\n",
      "    stability_temperature = 0.1\n",
      "    velocity_temperature = 0.1\n",
      "    \n",
      "    # Normalize stability penalty with e^(-penalty)\n",
      "    stability_penalty_normalized = -np.exp(-stability_penalty / stability_temperature)\n",
      "\n",
      "    # Aggregate rewards and penalties\n",
      "    reward = velocity_reward * velocity_temperature + stability_penalty_normalized\n",
      "    \n",
      "    # Reward components dictionary\n",
      "    reward_components = {\n",
      "        \"velocity_reward\": velocity_reward * velocity_temperature,\n",
      "        \"stability_penalty_normalized\": stability_penalty_normalized\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "Task 3 Name\n",
      "Goal-oriented Locomotion\n",
      "\n",
      "Task 3 Description\n",
      "Minimize the goal_distance, directing the agent towards the random goal_pos while maintaining the torso_orientation and torso_angular_velocity as stable as possible. Use the locomotion skills from Task 1 and stabilization from Task 2.\n",
      "\n",
      "```python\n",
      "def compute_goal_oriented_locomotion_reward(self) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    ant_obs = self.ant_env._get_obs()\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    orientation_penalty = np.linalg.norm(self.torso_orientation(ant_obs) - np.array([0.0, 0.75, 1.0]))\n",
      "    stability_penalty = np.linalg.norm(self.torso_angular_velocity(ant_obs))\n",
      "    \n",
      "    # Temperature parameters for scaling\n",
      "    goal_temperature = 10.0\n",
      "    orientation_temperature = 0.5\n",
      "    stability_temperature = 0.5\n",
      "    \n",
      "    # Calculate the normalized distance reward\n",
      "    distance_reward = np.exp(-distance_to_goal / goal_temperature)\n",
      "    \n",
      "    # Normalize penalties\n",
      "    orientation_penalty_normalized = -np.exp(-orientation_penalty / orientation_temperature)\n",
      "    stability_penalty_normalized = -np.exp(-stability_penalty / stability_temperature)\n",
      "\n",
      "    # Aggregate rewards and penalties\n",
      "    reward = distance_reward + orientation_penalty_normalized + stability_penalty_normalized\n",
      "    \n",
      "    # Reward components dictionary\n",
      "    reward_components = {\n",
      "        \"distance_reward\": distance_reward,\n",
      "        \"orientation_penalty_normalized\": orientation_penalty_normalized,\n",
      "        \"stability_penalty_normalized\": stability_penalty_normalized\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "Task 4 Name\n",
      "Maze Navigation\n",
      "\n",
      "Task 4 Description\n",
      "Navigate towards the goal_pos while avoiding walls and obstacles, keeping the torso_coordinate within the maze boundaries. Apply the locomotion and stabilizing skills acquired in previous tasks to handle the maze's complexity.\n",
      "\n",
      "```python\n",
      "def compute_maze_navigation_reward(self) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    ant_obs = self.ant_env._get_obs()\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    stability_penalty = np.linalg.norm(self.torso_angular_velocity(ant_obs))\n",
      "    body_pos = self.torso_coordinate(ant_obs)\n",
      "    \n",
      "    # Define the boundaries of the maze\n",
      "    maze_boundaries = self.maze_size\n",
      "\n",
      "    # Calculate the distance from boundaries (clamped at a minimum of 0)\n",
      "    boundary_distances = np.clip(maze_boundaries - np.abs(body_pos[:2]), 0, np.inf)\n",
      "    boundary_penalty = -np.sum(np.exp(-boundary_distances))  # Penalty for getting close to the boundaries\n",
      "    \n",
      "    # Temperature parameters for scaling\n",
      "    goal_temperature = 10.0\n",
      "    stability_temperature = 0.5\n",
      "    boundary_temperature = 1.0\n",
      "    \n",
      "    # Normalize the distance to goal and the penalties\n",
      "    distance_reward = np.exp(-distance_to_goal / goal_temperature)\n",
      "    stability_penalty_normalized = -np.exp(-stability_penalty / stability_temperature)\n",
      "    boundary_penalty_normalized = boundary_penalty / boundary_temperature\n",
      "\n",
      "    # Aggregate rewards and penalties\n",
      "    reward = distance_reward + stability_penalty_normalized + boundary_penalty_normalized\n",
      "    \n",
      "    # Reward components dictionary\n",
      "    reward_components = {\n",
      "        \"distance_reward\": distance_reward,\n",
      "        \"stability_penalty_normalized\": stability_penalty_normalized,\n",
      "        \"boundary_penalty_normalized\": boundary_penalty_normalized\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "Task 5 Original Task\n",
      "Effective Maze Solver\n",
      "\n",
      "Task 5 Original Task Description\n",
      "Maintain a goal_distance of 0.45, effectively reaching the target in the closed maze using the skills developed from all previous tasks.\n",
      "\n",
      "```python\n",
      "def compute_effective_maze_solver_reward(self) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    ant_obs = self.ant_env._get_obs()\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Define the ideal distance to be near the goal but not necessarily at the exact point\n",
      "    ideal_distance = 0.45\n",
      "\n",
      "    # Apply a heavy penalty if the distance is greater than the ideal distance\n",
      "    if distance_to_goal > ideal_distance:\n",
      "        distance_reward = -distance_to_goal\n",
      "    else:\n",
      "        # If within the ideal range, small positive reward for staying near the goal\n",
      "        distance_reward = ideal_distance - distance_to_goal\n",
      "\n",
      "    # Include previous tasks parameters\n",
      "    orientation_penalty = np.linalg.norm(self.torso_orientation(ant_obs) - np.array([0.0, 0.75, 1.0]))\n",
      "    stability_penalty = np.linalg.norm(self.torso_angular_velocity(ant_obs))\n",
      "\n",
      "    # Temperature parameters for scaling\n",
      "    distance_temperature = 1.0\n",
      "    orientation_temperature = 0.5\n",
      "    stability_temperature = 0.5\n",
      "    \n",
      "    # Normalize penalties\n",
      "    orientation_penalty_normalized = -np.exp(-orientation_penalty / orientation_temperature)\n",
      "    stability_penalty_normalized = -np.exp(-stability_penalty / stability_temperature)\n",
      "\n",
      "    # Aggregate rewards and penalties\n",
      "    reward = (distance_reward / distance_temperature) + orientation_penalty_normalized + stability_penalty_normalized\n",
      "    \n",
      "    # Reward components dictionary\n",
      "    reward_components = {\n",
      "        \"distance_reward\": distance_reward / distance_temperature,\n",
      "        \"orientation_penalty_normalized\": orientation_penalty_normalized,\n",
      "        \"stability_penalty_normalized\": stability_penalty_normalized\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "reward_system = file_to_string('./gpt/prompt/ant_maze/env_specific_reward_system.txt')\n",
    "reward_user = file_to_string('./gpt/prompt/ant_maze/env_specific_reward_user.txt')\n",
    "\n",
    "reward_user = reward_user + \"\\n\" + curriculum_answer\n",
    "\n",
    "reward_functions = gpt_interaction(reward_system, reward_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string_to_file(save_path=\"./gpt/prompt/ant_maze/env_specific_reward_4.md\", string_file=reward_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find statistics that is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required variables: [x_velocity_of_the_torso, healthy_reward, control_cost, z_coordinate_of_the_torso, angle_of_the_torso]\n",
      "[x_velocity_of_the_torso]: [Maximized]\n",
      "[healthy_reward]: [Maximized]\n",
      "[control_cost]: [Minimized]\n",
      "[z_coordinate_of_the_torso]: [Close to [0.7]]\n",
      "[angle_of_the_torso]: [Close to [0.0]]\n",
      "\n",
      "Reason:\n",
      "The x velocity of the torso should be maximized in order to achieve the highest forward speed.\n",
      "\n",
      "The healthy reward should be maximized because it indicates that the hopper is maintaining its height above 0.7 and keeping the torso angle within the desired range, which corresponds to being upright.\n",
      "\n",
      "Control cost should be minimized to ensure that the actions taken by the hopper, in terms of torques applied to hinges, are not excessively large, leading to more efficient and controlled hops.\n",
      "\n",
      "The z coordinate of the torso should be close to 0.7, as it directly correlates with the constraint mentioned in the healthy reward description, implying the hopper is airborne or maintaining sufficient height while hopping.\n",
      "\n",
      "The angle of the torso should be close to 0.0 radians indicating that the torso is upright, which is critical for maintaining balance and achieving forward momentum without falling.\n"
     ]
    }
   ],
   "source": [
    "system = file_to_string('./gpt/prompt/test/statistics_system.txt')\n",
    "user = file_to_string('./gpt/prompt/test/statistics_user.txt')\n",
    "\n",
    "statistics_answer = gpt_interaction(system, user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_healthy(observation):\n",
    "    z, angle = observation[1:3]\n",
    "    state = observation[2:]\n",
    "\n",
    "    min_state, max_state = (-100, 100)\n",
    "    min_z, max_z = (0.7, np.inf)\n",
    "    min_angle, max_angle = (-0.2, 0.2)\n",
    "\n",
    "    healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\n",
    "    healthy_z = min_z < z < max_z\n",
    "    healthy_angle = min_angle < angle < max_angle\n",
    "\n",
    "    is_healthy = all((healthy_state, healthy_z, healthy_angle))\n",
    "\n",
    "    return int(is_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/stand_still_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "# Get average of the is_healthy\n",
    "healthy = np.array([is_healthy(observation) for observation in obs])\n",
    "healthy_avg = np.mean(healthy)\n",
    "\n",
    "stand_still_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std) + \"\\nAverage is healthy: \" + str(healthy_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/simple_hopping_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "# Get average of the is_healthy\n",
    "healthy = np.array([is_healthy(observation) for observation in obs])\n",
    "healthy_avg = np.mean(healthy)\n",
    "\n",
    "simple_hopping_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std) + \"\\nAverage is healthy: \" + str(healthy_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 1\n",
      "Reason: \n",
      "\n",
      "Agent 1 exhibits no horizontal movement as the average and standard deviation of x position and velocity are both zero, which perfectly adheres to the task requirement of not moving forward or backward. Furthermore, Agent 1 has a high average z position, more consistent hopping height (lower standard deviation of z position), and maintains the healthy status throughout the trajectory. While the standard deviation of z velocity is non-zero, which indicates variability in the hopping speed, there's no displacement in the x-direction, so the trajectory meets the task requirements effectively. In contrast, Agent 2 has shown drift in the x-direction as indicated by the non-zero average x position and significant standard deviation of x position, suggesting it's not strictly hopping in place. The high standard deviation in x velocity for Agent 2 also demonstrates inconsistency in maintaining a stationary hop. Even though Agent 2 has a higher average z position, the deviation from the required task in the x-direction makes it less suitable than Agent 1.\n"
     ]
    }
   ],
   "source": [
    "assistant = statistics_answer\n",
    "\n",
    "user_2 = file_to_string('./gpt/prompt/test/statistics_user_2.txt')\n",
    "user_2 = user_2 + \"\\nAgent 1\" + stand_still_info + \"\\nAgent 2\" + simple_hopping_info\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": user},\n",
    "    {\"role\": \"assistant\", \"content\": assistant},\n",
    "    {\"role\": \"user\", \"content\": user_2},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
