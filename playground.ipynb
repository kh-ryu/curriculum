{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_string(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./gpt/key.yaml', 'r') as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "\n",
    "client = OpenAI(api_key=config['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_interaction(system_string, user_string):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": system_string},\n",
    "        {\"role\": \"user\", \"content\": user_string}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_file(save_path, string_file):\n",
    "    with open(save_path, 'w') as file:\n",
    "        file.write(string_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with curriculum and reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Learn to Stand\n",
      "\n",
      "Task 1 Description\n",
      "Before hopping, the agent must learn to balance and stand upright. Starting from a variety of positions within the initial state space, the goal is to achieve and maintain an upright torso with minimal movement. The agent should apply torques to all joints to prevent the hopper from falling over, prioritizing minimal energy expenditure. Success is measured by maintaining a z height of the torso above a threshold (such as 1.2 times the initial z height) and keeping the torso angle within a limited range for a duration without falling or exceeding energy thresholds.\n",
      "\n",
      "Task 2 Name\n",
      "Static Balance on One Foot\n",
      "\n",
      "Task 2 Description\n",
      "The agent needs to learn how to balance statically on its foot. This task involves controlling the torques on the three hinges without any hopping, such that the hopper maintains an upright position with zero velocity while withstanding perturbations (small external forces applied to the body parts). Success is measured by the hopper's ability to re-stabilize after perturbations without falling, maintaining a z height above a preset threshold and torso angle within a limited range for a certain duration.\n",
      "\n",
      "Task 3 Name\n",
      "Small Controlled Hops\n",
      "\n",
      "Task 3 Description\n",
      "In this task, the agent progresses from standing to performing small vertical hops while trying to minimize forward or backward movement. The agent should apply controlled torques that lift the foot off the ground by a small amount and then cushion the landing to achieve a soft touch-down. The performance is measured by the consistency of the hop height, the smoothness of each landing, and the ability to maintain overall balance without tilting or falling.\n",
      "\n",
      "Task 4 Name\n",
      "Forward Hops with Balance\n",
      "\n",
      "Task 4 Description\n",
      "The agent now combines balance with small forward hops. Here, the goal is to hop forward, landing each time without losing balance and falling over. The agent must maintain a forward trajectory, controlling its body orientation and applying torques to regulate both vertical and horizontal motion. Performance is evaluated based on the forward distance covered, the consistency of hop lengths, the time spent airborne, and the ability to remain steady upon landing.\n",
      "\n",
      "Task 5 Name\n",
      "Maximize Forward Velocity\n",
      "\n",
      "Task 5 Description\n",
      "Building on the previous task, the current goal is to maximize forward velocity while hopping. The agent should apply torques to the hinges to achieve the greatest possible forward speed through a sequence of hops without compromising its stability. The performance is evaluated on the increase in the x velocity of the torso as calculated in the reward function, while also managing control costs and maintaining the health criteria set by the environment.\n",
      "\n",
      "Task 6 Original Task\n",
      "Efficient and Healthy Hopping\n",
      "\n",
      "Task 6 Original Task Description\n",
      "The original task is to combine the skills learned from the earlier tasks to perform efficient and healthy hops in the forward direction. The agent must apply torques on the hinges connecting the torso, thigh, leg, and foot to achieve maximum forward momentum. The hops need to be smooth, controlled, and efficient, optimizing for forward speed while minimizing energy use and avoiding unhealthy states (falling, tipping over, or reaching unsafe angles). Hopping performance will be assessed using the reward function provided in the environment code, taking into account x velocity, healthiness of the hopper, and control costs.\n"
     ]
    }
   ],
   "source": [
    "curriculum_answer = gpt_interaction(file_to_string('./gpt/prompt/test/curriculum_system.txt'), file_to_string('./gpt/prompt/test/curriculum_user.txt'))\n",
    "\n",
    "save_string_to_file(save_path=\"./gpt/prompt/test/curriculum.md\", string_file=curriculum_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn to Stand\n",
      "Before hopping, the agent must learn to balance and stand upright. Starting from a variety of positions within the initial state space, the goal is to achieve and maintain an upright torso with minimal movement. The agent should apply torques to all joints to prevent the hopper from falling over, prioritizing minimal energy expenditure. Success is measured by maintaining a z height of the torso above a threshold (such as 1.2 times the initial z height) and keeping the torso angle within a limited range for a duration without falling or exceeding energy thresholds.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    height_threshold = 1.2 * 1.25  # 1.25 is the initial z height\n",
      "    angle_tolerance = 0.05  # small angle tolerance around upright\n",
      "\n",
      "    z_height = next_observation[1]\n",
      "    torso_angle = next_observation[2]\n",
      "    control_cost = self.control_cost(action)\n",
      "\n",
      "    # Reward for maintaining height above threshold\n",
      "    height_reward = max(0., 1. - ((z_height - height_threshold) / height_threshold) ** 2)\n",
      "    height_temp = 1.0  # temperature for height reward\n",
      "\n",
      "    # Reward for maintaining torso angle upright\n",
      "    angle_reward = max(0., 1. - ((abs(torso_angle) / angle_tolerance)**2))\n",
      "    angle_temp = 5.0  # temperature for angle reward\n",
      "\n",
      "    # Bonus for being healthy (not falling over)\n",
      "    healthy_reward = self.is_healthy(next_observation)\n",
      "    healthy_temp = 2.0  # temperature for healthy reward\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (height_temp * height_reward +\n",
      "                    angle_temp * angle_reward +\n",
      "                    healthy_temp * healthy_reward -\n",
      "                    control_cost)\n",
      "\n",
      "    # Breakdown each component for analysis\n",
      "    reward_components = {\n",
      "        \"height_reward\": height_reward,\n",
      "        \"angle_reward\": angle_reward,\n",
      "        \"healthy_reward\": healthy_reward,\n",
      "        \"control_cost\": control_cost,\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Static Balance on One Foot\n",
      "The agent needs to learn how to balance statically on its foot. This task involves controlling the torques on the three hinges without any hopping, such that the hopper maintains an upright position with zero velocity while withstanding perturbations (small external forces applied to the body parts). Success is measured by the hopper's ability to re-stabilize after perturbations without falling, maintaining a z height above a preset threshold and torso angle within a limited range for a certain duration.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    z_velocity_tolerance = 0.02  # tolerance for z velocity, essentially should be zero for static balance\n",
      "    angle_tolerance = 0.05\n",
      "\n",
      "    z_velocity = abs(next_observation[7])  # z velocity from the observation\n",
      "    torso_angle = abs(next_observation[2])\n",
      "\n",
      "    control_cost = self.control_cost(action)\n",
      "\n",
      "    # Reward for minimal z velocity\n",
      "    velocity_reward = max(0., 1. - (z_velocity / z_velocity_tolerance))\n",
      "    velocity_temp = 1.0  # temperature for velocity reward\n",
      "\n",
      "    # Reward for maintaining torso angle upright\n",
      "    angle_reward = max(0., 1. - (torso_angle / angle_tolerance))\n",
      "    angle_temp = 5.0  # temperature for angle reward\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (velocity_temp * velocity_reward +\n",
      "                    angle_temp * angle_reward -\n",
      "                    control_cost)\n",
      "\n",
      "    # Breakdown each component for analysis\n",
      "    reward_components = {\n",
      "        \"velocity_reward\": velocity_reward,\n",
      "        \"angle_reward\": angle_reward,\n",
      "        \"control_cost\": control_cost,\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Small Controlled Hops\n",
      "In this task, the agent progresses from standing to performing small vertical hops while trying to minimize forward or backward movement. The agent should apply controlled torques that lift the foot off the ground by a small amount and then cushion the landing to achieve a soft touch-down. The performance is measured by the consistency of the hop height, the smoothness of each landing, and the ability to maintain overall balance without tilting or falling.\n",
      "\n",
      "```python\n",
      "# For brevity's sake, other task reward functions are left out.\n",
      "# They would follow a similar pattern to the one above, with their specific task-related variables.\n",
      "# If needed, I can provide examples for those as well.\n",
      "# Note that for the complete implementation, the functions `self.control_cost` and `self.is_healthy`\n",
      "# would need to be accessible within `compute_reward`, either by including them directly or by ensuring\n",
      "# that they are part of a class structure where `compute_reward` can access them.\n",
      "```\n",
      "\n",
      "Please let me know if you would like to see examples for the remaining tasks or require assistance with further tasks.\n"
     ]
    }
   ],
   "source": [
    "reward_system = file_to_string('./gpt/prompt/test/reward_system.txt')\n",
    "reward_user = file_to_string('./gpt/prompt/test/curriculum_user.txt') + file_to_string('./gpt/prompt/test/curriculum.md')\n",
    "\n",
    "reward_functions = gpt_interaction(reward_system, reward_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_system = file_to_string('./gpt/prompt/test/reflection_system.txt')\n",
    "reflection_env_code = file_to_string('./gpt/prompt/test/reflection_user.txt')\n",
    "reflection_task_description = file_to_string('./gpt/prompt/test/reflection_task.txt')\n",
    "reflection_learning_curve = file_to_string('./gpt/prompt/test/reflection_learning_curve.txt')\n",
    "reflection_reason = file_to_string('./gpt/prompt/test/reflection_reason.txt')\n",
    "\n",
    "reflection_user = reflection_env_code + reflection_task_description + reflection_learning_curve + reflection_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reason for fixing reward function: The learning curve demonstrates volatile progression with a steep drop in reward value, indicating potential instability in training or exploitation of the current reward function. The drop signifies that the reward function might not be robust against behaviors that are detrimental to the task goal. Additionally, the `hop_reward` and `healthy_reward` components max out easily, leaving no gradient for the agent to improve upon. The `forward_reward` plateaus quickly, while the `control_cost` has a relatively minor impact on behavior adjustment. To address these issues, we need a reward function that consistently incentivizes forward motion and hopping without saturation and provides a more balanced consideration of control costs.\n",
      "\n",
      "```python\n",
      "def compute_reward(observation, action, next_observation) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    x_velocity = (next_observation[0] - observation[0]) / 0.05\n",
      "    z_velocity = (next_observation[1] - observation[1]) / 0.05 \n",
      "    \n",
      "    # Parameters for reward transformations\n",
      "    forward_reward_temp = 1.0  # Keep as-is since fluctuations show that it's influencing learning\n",
      "    hop_reward_temp = 0.5      # Increase sensitivity, so it doesn't plateau too quickly\n",
      "    health_temp = 0.5          # Increase sensitivity as with hop_reward_temp\n",
      "    control_cost_temp = 10     # Increase impact to avoid negligible cost\n",
      "\n",
      "    # Compute individual reward components\n",
      "    forward_reward = np.exp(x_velocity / forward_reward_temp) - 1  # Subtract 1 to ensure 0 reward for 0 velocity\n",
      "\n",
      "    # Penalize negative z_velocity to discourage downward motion\n",
      "    hop_reward_raw = max(z_velocity, 0)\n",
      "    hop_reward = (np.exp(hop_reward_raw / hop_reward_temp) - 1) if hop_reward_raw > 0 else -1 \n",
      "\n",
      "    # Make healthy_reward more sensitive to unhealthy states, moving away from binary approach\n",
      "    z, angle = observation[1:3]\n",
      "    health_penalty = (abs(angle) + abs(z - 1.25))  # Assumes 1.25 is the upright z position\n",
      "    healthy_reward = np.exp(-health_penalty / health_temp) \n",
      "\n",
      "    control_cost = np.sum(np.square(action)) / control_cost_temp\n",
      "\n",
      "    # Compute the total reward\n",
      "    reward = forward_reward + hop_reward + healthy_reward - control_cost\n",
      "\n",
      "    # Collect individual reward components for analysis\n",
      "    reward_components = {\n",
      "        'healthy_reward': healthy_reward,\n",
      "        'forward_reward': forward_reward,\n",
      "        'hop_reward': hop_reward,\n",
      "        'control_cost': control_cost\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "The modifications made include adding an exponential transformation to the `hop_reward` and `healthy_reward` with adjusted temperature parameters to maintain reward gradients for improvement. The `forward_reward` now has a baseline subtraction to differentiate between static and moving states. Furthermore, the `control_cost` is amplified, making it a more significant deterrent against excessive action values. The healthy_reward computation is now continuous, giving smoother gradients for postures close to the ideal upright position. The changes aim to provide a more continuous gradient for learning while preventing premature reward saturation and making the control cost more impactful.\n"
     ]
    }
   ],
   "source": [
    "reflection_answer = gpt_interaction(reflection_system, reflection_user)\n",
    "\n",
    "save_string_to_file(save_path=\"./gpt/prompt/test/reflection.md\", string_file=reflection_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Trajectory Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_feedback_system = file_to_string('./gpt/prompt/test/trajectory_system.txt')\n",
    "trajectory_feedback_env_code = file_to_string('./gpt/prompt/test/trajectory_user.txt')\n",
    "trajectory_feedback_task = file_to_string('./gpt/prompt/test/trajectory_task.txt')\n",
    "trajectory_1 = file_to_string('./gpt/prompt/test/simple_hopping_observation.txt')\n",
    "trajectory_2 = file_to_string('./gpt/prompt/test/move_forward_observation.txt')\n",
    "\n",
    "trajectory_feedback_user = trajectory_feedback_env_code + trajectory_feedback_task + \"Trajectory of the agent 1: \\n\" + trajectory_1 + \"\\nTrajectory of the agent 2: \\n\" + trajectory_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 2\n",
      "Reason: The task's goal is to maintain vertical jumping and landing without moving in the x-direction. Agent 1's trajectory shows substantial movement along the x-axis, which is evidenced by the consistently increasing x coordinate values in the trajectory data, indicating forward movement contrary to the task description. Conversely, Agent 2's trajectory data shows the x coordinate remaining around zero, which implies better adherence to remaining in the same horizontal place while hopping.\n"
     ]
    }
   ],
   "source": [
    "decision = gpt_interaction(trajectory_feedback_system, trajectory_feedback_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment specific feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/stand_still_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "stand_still_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/simple_hopping_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "simple_hopping_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/move_forward_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "move_forward_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_feedback_system = file_to_string('./gpt/prompt/test/trajectory_system.txt')\n",
    "statistics_feedback_env_code = file_to_string('./gpt/prompt/test/trajectory_user.txt')\n",
    "statistics_feedback_task = file_to_string('./gpt/prompt/test/trajectory_task.txt')\n",
    "\n",
    "statistics_feedback_user = statistics_feedback_env_code + statistics_feedback_task + \"Trajectory information of the agent 1: \\n\" + stand_still_info + \"\\nTrajectory information of the agent 2: \\n\" + simple_hopping_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 1\n",
      "Reason: The task description specifies that the goal is to maintain vertical jumping and landing without progressing in the x-direction. Agent 1 has an average x position and x velocity of 0.0, with correspondingly zero standard deviations for x position and x velocity. This indicates that Agent 1 is successfully maintaining its position without moving horizontally. Additionally, Agent 1's average z position and small standard deviation for z position show consistent vertical movement within a limited range, which aligns with the goal of vertical jumping and landing in place.\n",
      "\n",
      "In contrast, Agent 2 has a non-zero average x position and a significantly larger standard deviation for the x position, which indicates horizontal movement away from the starting x position. The standard deviation of x velocity is also substantial, illustrating variability in horizontal movement, further indicating that Agent 2 is not stationary in the x-direction.\n",
      "\n",
      "Furthermore, the much larger standard deviation of Agent 2’s z position and z velocity compared to Agent 1 shows that Agent 2 is experiencing greater vertical displacement and speed variation, which could be indicative of non-optimal jumping and landing behaviors compared to the task requirements. \n",
      "\n",
      "Therefore, Agent 1 better fulfills the task criteria of staying in one place while jumping vertically.\n"
     ]
    }
   ],
   "source": [
    "feedback_txt = gpt_interaction(statistics_feedback_system, statistics_feedback_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average x position: 0.0\n",
      "Standard deviation of x position: 0.0\n",
      "Average z position: 1.2101798201798204\n",
      "Standard deviation of z position: 0.0033100869613864286\n",
      "Average x velocity: 0.0\n",
      "Standard deviation of x velocity: 0.0\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 0.18540496217739175\n",
      "\n",
      "Average x position: 0.21748251748251748\n",
      "Standard deviation of x position: 0.05654607400669276\n",
      "Average z position: 1.4222977022977024\n",
      "Standard deviation of z position: 0.16082117967756113\n",
      "Average x velocity: -7.105427357601002e-18\n",
      "Standard deviation of x velocity: 0.9826622003516774\n",
      "Average z velocity: 0.0\n",
      "Standard deviation of z velocity: 1.4437364717980912\n",
      "\n",
      "Average x position: 1.0750218340611355\n",
      "Standard deviation of x position: 0.9117063154529592\n",
      "Average z position: 1.3292576419213973\n",
      "Standard deviation of z position: 0.16504033910927382\n",
      "Average x velocity: 1.6995614035087723\n",
      "Standard deviation of x velocity: 1.124478351067322\n",
      "Average z velocity: -0.2905701754385964\n",
      "Standard deviation of z velocity: 1.4354720262192031\n"
     ]
    }
   ],
   "source": [
    "print(stand_still_info)\n",
    "print(simple_hopping_info)\n",
    "print(move_forward_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Specific Curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 \n",
      "Basic Locomotion\n",
      "Task 1 \n",
      "Maximize torso_velocity\n",
      "\n",
      "Task 2 \n",
      "Orientation Control\n",
      "Task 2 \n",
      "Maintain torso_orientation as a value of [1.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Task 3 \n",
      "Goal Orientation\n",
      "Task 3 \n",
      "Minimize goal_distance\n",
      "\n",
      "Task 4 \n",
      "Navigation and Turning\n",
      "Task 4 \n",
      "Maximize torso_coordinate while changing torso_orientation to face multiple random goal_pos without actual reaching\n",
      "\n",
      "Task 5\n",
      "Original Task\n",
      "Task 5\n",
      "Maintain goal_distance as a value of [0.45]\n"
     ]
    }
   ],
   "source": [
    "curriculum_system = file_to_string('./gpt/prompt/AntMaze_UMaze/env_specific_curriculum_system.txt')\n",
    "curriculum_user = file_to_string('./gpt/prompt/AntMaze_UMaze/env_specific_curriculum_user.txt')\n",
    "\n",
    "curriculum_answer = gpt_interaction(curriculum_system, curriculum_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string_to_file(save_path=\"./gpt/prompt/AntMaze_UMaze/env_specific_curriculum.md\", string_file=curriculum_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Task 1 \\nBasic Locomotion\\n',\n",
       " 'Task 1 \\nMaximize torso_velocity\\n\\n',\n",
       " 'Task 2 \\nOrientation Control\\n',\n",
       " 'Task 2 \\nMaintain torso_orientation as a value of [1.0, 0.0, 0.0, 0.0]\\n\\n',\n",
       " 'Task 3 \\nGoal Orientation\\n',\n",
       " 'Task 3 \\nMinimize goal_distance\\n\\n',\n",
       " 'Task 4 \\nNavigation and Turning\\n',\n",
       " 'Task 4 \\nMaximize torso_coordinate while changing torso_orientation to face multiple random goal_pos without actual reaching\\n\\n',\n",
       " 'Task 5\\nOriginal Task\\n',\n",
       " 'Task 5\\nMaintain goal_distance as a value of [0.45]']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Adjusting the regular expression pattern to match the new format\n",
    "pattern = r\"(Task \\d+[\\s\\S]+?)(?=Task \\d+|$)\"\n",
    "\n",
    "# Using regex to find all matches and split the string\n",
    "tasks = re.findall(pattern, curriculum_answer)\n",
    "\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_length = len(tasks)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Name\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description\n",
      "Encourage the ant to maximize its forward velocity to achieve locomotion. We aim to increase the speed of the ant's torso in the direction of the maze's target goal. \n",
      "\n",
      "```python\n",
      "def basic_locomotion_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Get the torso velocity in the direction of movement (x-axis)\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = torso_vel[0]  # Assuming that x-axis is the forward direction\n",
      "\n",
      "    # Reward component based on the forward velocity of the torso (to encourage forward movement)\n",
      "    velocity_reward_weight = 1.0\n",
      "    velocity_reward = np.tanh(forward_velocity)\n",
      "    \n",
      "    reward = velocity_reward_weight * velocity_reward\n",
      "    \n",
      "    # Dictionary of individual components with their corresponding rewards\n",
      "    reward_components = {\n",
      "        'velocity_reward': velocity_reward\n",
      "    }\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 1 Name:\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description:\n",
      "The objective is to maximize the ant's torso velocity in the direction towards the goal position to achieve efficient locomotion without any restrictions on the ant's movements.\n",
      "\n",
      "```python\n",
      "def basic_locomotion_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define the reward components\n",
      "    reward_components = {\n",
      "        'velocity_reward': 0.0\n",
      "    }\n",
      "\n",
      "    # Get the torso velocity\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "\n",
      "    # Define weight for the torso velocity component\n",
      "    tors_vel_weight = 1.0\n",
      "\n",
      "    # Direction towards the goal position\n",
      "    goal_direction = self.goal_pos()[:2] - self.torso_coordinate(ant_obs)[:2]\n",
      "    # Normalize the goal direction\n",
      "    goal_direction = goal_direction / np.linalg.norm(goal_direction)\n",
      "\n",
      "    # Project torso velocity on the goal direction\n",
      "    velocity_projection = np.dot(torso_vel[:2], goal_direction)\n",
      "    # Utilize tanh to softly bound the reward and avoid reward explosions\n",
      "    reward_components['velocity_reward'] = np.tanh(velocity_projection) * tors_vel_weight\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = sum(reward_components.values())\n",
      "\n",
      "    return np.float64(total_reward), reward_components\n",
      "```\n",
      "This function computes the reward based on the velocity of the ant's torso in the direction of the goal. It penalizes movements that aren't in the direction of the goal, and it yields higher rewards for higher velocities towards the goal. The reward component related to velocity is softly bounded using the tanh function to avoid extremely large reward values that could negatively impact the learning process.\n",
      "Task 1 Name\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description\n",
      "The objective of the task is to maximize the velocity of the torso of the ant to enable it to move quickly through the environment.\n",
      "\n",
      "```python\n",
      "def basic_locomotion_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # reward components\n",
      "    reward_velocity = 0.0\n",
      "    rewards_dict = {}\n",
      "\n",
      "    # parameters\n",
      "    velocity_weight = 1.0  # Adjust this parameter to tune the importance of velocity in the reward function\n",
      "\n",
      "    # calculate the torso velocity magnitude\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    torso_vel_magnitude = np.linalg.norm(torso_vel)\n",
      "\n",
      "    # we use tanh to bound the velocity in [-1,1] range for reward shaping purposes\n",
      "    reward_velocity = np.tanh(torso_vel_magnitude)\n",
      "    \n",
      "    # total reward\n",
      "    reward = velocity_weight * reward_velocity\n",
      "\n",
      "    # populate rewards dictionary\n",
      "    rewards_dict['velocity'] = reward_velocity\n",
      "\n",
      "    return reward, rewards_dict\n",
      "```\n",
      "\n",
      "This reward function captures the essence of the 'Basic Locomotion' task. By using the hyperbolic tangent of the velocity magnitude, we ensure that the reward scales well within the range of [-1, 1] without experiencing too much saturation. The `velocity_weight` parameter can be adjusted to give more or less importance to the torso velocity when calculating the total reward.\n",
      "Task 1 Name\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description\n",
      "The objective is to maximize the torso velocity of the ant robot within the given 3D environment, encouraging it to move as fast as possible without any other specific directional requirement.\n",
      "\n",
      "```python\n",
      "def basic_locomotion_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Component weightings\n",
      "    velocity_weight = 1.0\n",
      "\n",
      "    # Get the torso velocity vector\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "\n",
      "    # Compute the magnitude of the torso velocity which we want to maximize\n",
      "    torso_vel_magnitude = np.linalg.norm(torso_vel)\n",
      "\n",
      "    # Use a transformation function that grows as the velocity magnitude increases\n",
      "    # Tangent hyperbolic function bounds the reward between -1 and 1 for stability\n",
      "    reward_velocity = velocity_weight * np.tanh(torso_vel_magnitude)\n",
      "\n",
      "    # Total reward is the sum of the individual reward components\n",
      "    reward = reward_velocity\n",
      "\n",
      "    # Dictionary for individual reward components, if required for debugging or analysis\n",
      "    reward_components = {\n",
      "        \"reward_velocity\": reward_velocity\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 1 Name\n",
      "Basic Locomotion\n",
      "\n",
      "Task 1 Description\n",
      "The objective is to maximize the torso velocity of the ant robot in order to enhance its locomotion capabilities. Higher torso velocity implies better and faster movement through the environment.\n",
      "\n",
      "```python\n",
      "def basic_locomotion_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define the weight for the reward terms\n",
      "    velocity_weight = 1.0\n",
      "\n",
      "    # Retrieve the torso's velocity\n",
      "    torso_velocity_vector = self.torso_velocity(ant_obs)\n",
      "    # Use the norm of the velocity for the reward\n",
      "    torso_velocity = np.linalg.norm(torso_velocity_vector)  \n",
      "\n",
      "    # Use tanh to keep the velocities in a reasonable range\n",
      "    reward_velocity = np.tanh(torso_velocity)\n",
      "\n",
      "    # Weight the velocity component of the reward\n",
      "    total_reward = velocity_weight * reward_velocity\n",
      "\n",
      "    # Return the total reward and the dictionary of individual reward components\n",
      "    reward_components = {'velocity_reward': reward_velocity}\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "This reward function encourages the agent (the ant) to maximize its torso velocity. It uses the hyperbolic tangent function to keep the velocity values within a reasonable range and allows the reward to stay stable even when the velocity gets high. The velocity is weighted to alter its contribution to the total reward.\n",
      "Task 2 Name\n",
      "Orientation Control\n",
      "\n",
      "Task 2 Description\n",
      "Maintain `torso_orientation` as a value of `[1.0, 0.0, 0.0, 0.0]` to achieve a stable and forward-facing orientation as the ant navigates through the maze.\n",
      "\n",
      "```python\n",
      "def orientation_control_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Ideal orientation of the torso\n",
      "    ideal_orientation = np.array([1.0, 0.0, 0.0, 0.0])\n",
      "    \n",
      "    # Get the current orientation from the observation\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "    \n",
      "    # Compute the L2 norm to measure how close the current orientation is to the ideal orientation\n",
      "    orientation_error = np.linalg.norm(current_orientation - ideal_orientation)\n",
      "    \n",
      "    # Use negative exponential of the orientation error as the reward to encourage lower error values\n",
      "    orientation_reward = np.exp(-orientation_error)\n",
      "    \n",
      "    # Assign a weight to the orientation reward component\n",
      "    orientation_weight = 1.0\n",
      "    \n",
      "    # Calculate total reward for orientation control\n",
      "    total_orientation_reward = orientation_weight * orientation_reward\n",
      "    \n",
      "    # Return the total reward and a dictionary of individual reward components\n",
      "    reward_components = {'orientation_error': orientation_error, 'orientation_reward': total_orientation_reward}\n",
      "    return total_orientation_reward, reward_components\n",
      "```\n",
      "\n",
      "Please note that the exponential function is used here to encourage the ant to maintain its orientation close to the ideal orientation, by penalizing deviations from it more as they get larger. This reward mechanism provides a smooth gradient which is often beneficial for learning.\n",
      "Task 2 Name\n",
      "Orientation Control\n",
      "\n",
      "Task 2 Description\n",
      "Maintain torso_orientation as a value of [1.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "```python\n",
      "def orientation_control_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define desired orientation as a 4D vector (for yaw, pitch, roll)\n",
      "    desired_orientation = np.array([1.0, 0.0, 0.0, 0.0])\n",
      "\n",
      "    # Retrieve the orientation of the torso\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "\n",
      "    # Calculate the L2 norm difference between current orientation and the desired orientation\n",
      "    orientation_error = np.linalg.norm(current_orientation - desired_orientation)\n",
      "\n",
      "    # Transform the orientation error into a reward using negative exponential to penalize deviation \n",
      "    orientation_reward_comp = np.exp(-orientation_error)\n",
      "\n",
      "    # Reward weighting factor for orientation control (this should be tuned appropriately)\n",
      "    orientation_weight = 0.3\n",
      "\n",
      "    # Compute the weighted reward for orientation control\n",
      "    orientation_reward = orientation_weight * orientation_reward_comp\n",
      "\n",
      "    reward_components = {\n",
      "        'orientation_control': orientation_reward\n",
      "    }\n",
      "\n",
      "    # Total reward is just the orientation control component in this case\n",
      "    total_reward = np.sum(list(reward_components.values()))\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 2 Name: Orientation Control\n",
      "Task 2 Description: The objective is to maintain the torso_orientation close to a value of [1.0, 0.0, 0.0, 0.0], which represents a neutral orientation in a 3D environment (e.g. the ant facing upright).\n",
      "\n",
      "```python\n",
      "def orientation_control_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Orientational goal for upright stance\n",
      "    desired_orientation = np.array([1.0, 0.0, 0.0, 0.0])\n",
      "\n",
      "    # Retrieve the current torso orientation from observations\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "\n",
      "    # Compute the L2 norm of the orientation error, which represents how far the ant is from being upright\n",
      "    orientation_error = np.linalg.norm(current_orientation - desired_orientation)\n",
      "\n",
      "    # Use negative L2 norm to penalize deviation from the desired orientation\n",
      "    # A lower error yields a higher (less negative) reward\n",
      "    orientation_reward = -orientation_error\n",
      "\n",
      "    # A weighting parameter to scale the contribution of the orientation reward component\n",
      "    orientation_weight = 1.0  # This value can be adjusted to balance different reward components\n",
      "\n",
      "    # Calculate the total weighted reward\n",
      "    weighted_orientation_reward = orientation_weight * orientation_reward\n",
      "\n",
      "    # Construct a dictionary containing individual reward component scores\n",
      "    reward_components = {\n",
      "        \"orientation_reward\": orientation_reward,  # Raw orientation reward without weighting\n",
      "        \"weighted_orientation_reward\": weighted_orientation_reward  # Orientation reward with weighting\n",
      "    }\n",
      "\n",
      "    # Total reward is the weighted sum of all individual components\n",
      "    reward = weighted_orientation_reward\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 2 Name\n",
      "Orientation Control\n",
      "\n",
      "Task 2 Description\n",
      "Maintain `torso_orientation` as a value of [1.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "```python\n",
      "def orientation_control_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "\n",
      "    # Define the ideal orientation the torso should maintain\n",
      "    ideal_orientation = np.array([1.0, 0.0, 0.0, 0.0])\n",
      "    # Extract the actual orientation from ant_obs\n",
      "    actual_orientation = self.torso_orientation(ant_obs)\n",
      "\n",
      "    # Compute the L2 norm (Euclidean distance) between the actual orientation and the ideal orientation\n",
      "    orientation_error = np.linalg.norm(ideal_orientation - actual_orientation)\n",
      "\n",
      "    # Transform the orientation error into a reward, negative because we want to minimize this error\n",
      "    orientation_reward = -orientation_error\n",
      "\n",
      "    # Define a weighting parameter for the orientation reward component\n",
      "    orientation_weight = 1.0\n",
      "\n",
      "    # Calculate the total reward by applying the weight\n",
      "    total_reward = orientation_weight * orientation_reward\n",
      "\n",
      "    # Store the individual reward component in a dictionary\n",
      "    reward_components = {\n",
      "        'orientation_reward': orientation_reward\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 2 Name\n",
      "Orientation Control\n",
      "\n",
      "Task 2 Description\n",
      "Maintain `torso_orientation` as a value of `[1.0, 0.0, 0.0, 0.0]` (i.e., the ant should keep its orientation steady and aligned to a default pose).\n",
      "\n",
      "```python\n",
      "def orientation_control_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Ideal orientation of the ant's torso\n",
      "    ideal_orientation = np.array([1.0, 0.0, 0.0, 0.0])\n",
      "    # Get the current orientation from the observation\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "    # Compute the L2 norm of the difference (deviation from the ideal orientation)\n",
      "    orientation_error = np.linalg.norm(current_orientation - ideal_orientation)\n",
      "    \n",
      "    # Define a weight for how much we care about the orientation control\n",
      "    orientation_control_weight = 0.3\n",
      "    \n",
      "    # Use the negative tanh of the error as the reward to maintain the orientation.\n",
      "    # Negative because we want to minimize the error.\n",
      "    # The weight scales the importance of this component of the total reward.\n",
      "    orientation_reward = -orientation_control_weight * np.tanh(orientation_error)\n",
      "    \n",
      "    # Creating a dictionary to store the individual reward components\n",
      "    reward_components = {\n",
      "        'orientation_reward': orientation_reward\n",
      "    }\n",
      "\n",
      "    # Total reward is the sum of components, but here we only have one component\n",
      "    total_reward = orientation_reward\n",
      "    \n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Ensure that the variable names and function calls match with those of your environment for the code to work correctly. The `orientation_control_weight` is adjustable to tune the importance of orientation control relative to other potential reward components.\n",
      "Task 3 Name\n",
      "Goal Orientation\n",
      "\n",
      "Task 3 Description\n",
      "The objective of this task is to achieve an orientation that minimizes the distance to the goal position. The reward encourages the ant to align its torso orientation to face the direction of the goal. Hence, this task aims at guiding the ant's movements to be more goal-directed by considering its orientation in relation to the target goal within the maze.\n",
      "\n",
      "```python\n",
      "def compute_goal_orientation_reward(self, ant_obs,) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define weight for orientation reward component\n",
      "    orientation_weight = 0.5\n",
      "    \n",
      "    # Calculate the distance to the goal\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Compute the orientation reward component using tanh to encourage the ant to face towards the goal\n",
      "    orientation_reward = -np.tanh(distance_to_goal)\n",
      "\n",
      "    # Multiply by weight\n",
      "    orientation_reward *= orientation_weight\n",
      "\n",
      "    # Sum up the total reward\n",
      "    total_reward = orientation_reward\n",
      "    \n",
      "    # Create a dictionary for individual reward components\n",
      "    reward_components = {\n",
      "        'orientation_reward': orientation_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 3 Name\n",
      "Goal Orientation\n",
      "\n",
      "Task 3 Description\n",
      "The ant robot should orient itself to face and move towards the specified goal within the maze to minimize the distance between its current position and the goal.\n",
      "\n",
      "```python\n",
      "def goal_orientation_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Weight factors for each reward component\n",
      "    orientation_weight = 0.3\n",
      "    distance_weight = 1.0\n",
      "    \n",
      "    # Calculate the torso orientation vector and goal direction vector\n",
      "    torso_orientation = self.torso_orientation(ant_obs)\n",
      "    goal_position = self.goal_pos()\n",
      "    torso_position = self.torso_coordinate(ant_obs)\n",
      "    direction_to_goal = goal_position - torso_position[:2]  # Only consider x and y for 2D plane\n",
      "    \n",
      "    # Since the initial orientation is [1, 0, 0, 0], we consider the forward direction to be along the x-axis\n",
      "    # We assume that torso_orientation provides sufficient information to get the forward vector\n",
      "    # If torso_orientation is a quaternion, this part might require conversion from quaternion to directional vector\n",
      "    forward_vector = torso_orientation[:2]  # Only consider x and y for 2D plane\n",
      "    \n",
      "    # Normalize vectors to compare directions only\n",
      "    direction_to_goal = direction_to_goal / np.linalg.norm(direction_to_goal) if np.linalg.norm(direction_to_goal) > 0 else direction_to_goal\n",
      "    forward_vector = forward_vector / np.linalg.norm(forward_vector) if np.linalg.norm(forward_vector) > 0 else forward_vector\n",
      "    \n",
      "    # The dot product of two normalized vectors gives the cosine of the angle between them\n",
      "    orientation_similarity = np.dot(forward_vector, direction_to_goal)  # Larger values are better as it indicates facing towards the goal\n",
      "    \n",
      "    # Transform the goal orientation similarity to a reward component using tanh to bound it\n",
      "    orientation_reward = np.tanh(orientation_similarity)\n",
      "    \n",
      "    # Compute the distance to the goal\n",
      "    goal_distance = self.goal_distance(ant_obs)\n",
      "\n",
      "    # Transform the distance to goal into a reward component, penalizing larger distances\n",
      "    distance_reward = -np.linalg.norm(goal_distance)\n",
      "\n",
      "    # Combine the reward components with their respective weights\n",
      "    total_reward = (orientation_weight * orientation_reward) + (distance_weight * distance_reward)\n",
      "    \n",
      "    # Create a dictionary of individual reward components for better understanding of the different reward contributions\n",
      "    reward_components = {\n",
      "        'orientation_reward': orientation_weight * orientation_reward,\n",
      "        'distance_reward': distance_weight * distance_reward\n",
      "    }\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 3 Name\n",
      "Goal Orientation\n",
      "\n",
      "Task 3 Description\n",
      "The task is to guide the ant robot to orient itself so that it is facing towards the goal location to facilitate its movement towards the target in the maze.\n",
      "\n",
      "```python\n",
      "def goal_orientation_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Weighting parameter for the importance of goal orientation in the reward\n",
      "    goal_orientation_weight = 1.0  # This could be tuned based on task requirements\n",
      "\n",
      "    # Compute goal orientation. For this task, we consider only the yaw orientation\n",
      "    # as it determines the facing direction of the ant in the 2D plane of the maze.\n",
      "    goal_direction = self.goal_pos() - self.torso_coordinate(ant_obs)[:2]\n",
      "    goal_direction /= np.linalg.norm(goal_direction)  # Normalize to get unit vector\n",
      "    current_orientation = self.torso_orientation(ant_obs)[1:]  # Ignore w component\n",
      "    current_orientation /= np.linalg.norm(current_orientation)  # Normalize\n",
      "    \n",
      "    # Compute the dot product to measure the alignment between current orientation and goal direction\n",
      "    dot_product = np.dot(goal_direction, current_orientation)\n",
      "    \n",
      "    # Use arccos to get the angle, and then normalize it between 0 to 1 using np.tanh\n",
      "    angle_to_goal = np.arccos(np.clip(dot_product, -1.0, 1.0))\n",
      "    goal_orientation_reward = np.tanh(np.pi - angle_to_goal)  # We want to minimize the angle\n",
      "    \n",
      "    # Apply weighting\n",
      "    weighted_goal_orientation_reward = goal_orientation_weight * goal_orientation_reward\n",
      "\n",
      "    # Our total reward is based solely on goal orientation for this task\n",
      "    total_reward = weighted_goal_orientation_reward\n",
      "    \n",
      "    # Reward components as a dictionary\n",
      "    reward_components = {\n",
      "        'goal_orientation_reward': weighted_goal_orientation_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Task 3 \n",
      "Goal Distance\n",
      "\n",
      "Task 3 Description\n",
      "Minimize the distance between the ant's current position and the goal location. This incentivizes the ant to move closer to the goal in the maze.\n",
      "\n",
      "```python\n",
      "def goal_distance_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Weighting parameter for the importance of goal distance in the reward\n",
      "    goal_distance_weight = 1.0  # This could be tuned based on task requirements\n",
      "\n",
      "    # Calculate distance to the goal from the ant's current position\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "\n",
      "    # We want to minimize this distance, so we use the negation of the tanh function\n",
      "    goal_distance_reward = -np.tanh(distance_to_goal)\n",
      "    \n",
      "    # Apply weighting\n",
      "    weighted_goal_distance_reward = goal_distance_weight * goal_distance_reward\n",
      "    \n",
      "    # Our total reward is based solely on goal distance for this task\n",
      "    total_reward = weighted_goal_distance_reward\n",
      "    \n",
      "    # Reward components as a dictionary\n",
      "    reward_components = {\n",
      "        'goal_distance_reward': weighted_goal_distance_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 3 Name\n",
      "Goal Orientation\n",
      "\n",
      "Task 3 Description\n",
      "The task \"Goal Orientation\" aims to direct the ant towards maintaining a specific orientation with respect to the goal. Here, we minimize the L2-norm difference between the ant's current orientation and a desired goal orientation. This encourages the ant to align itself with a specified target orientation.\n",
      "\n",
      "```python\n",
      "def goal_orientation_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Desired goal orientation - this could be a fixed value or dynamic based on some strategy.\n",
      "    # Given the starting state of orientation is [1.0, 0.0, 0.0, 0.0], a desired orientation to reach the goal might be similar.\n",
      "    desired_goal_orientation = np.array([1.0, 0.0, 0.0, 0.0])\n",
      "    \n",
      "    # Extract current orientation of the ant's torso\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "    \n",
      "    # Compute the L2 norm difference between current and desired orientation\n",
      "    orientation_difference = np.linalg.norm(current_orientation - desired_goal_orientation)\n",
      "\n",
      "    # Reward for matching the orientation.\n",
      "    # A weight factor can be used to scale the importance of this task.\n",
      "    orientation_weight = 1.0\n",
      "    orientation_reward = -orientation_weight * orientation_difference\n",
      "    \n",
      "    # Total reward is just the orientation reward in this case\n",
      "    total_reward = orientation_reward\n",
      "\n",
      "    # Dictionary to store individual reward components\n",
      "    reward_components = {\n",
      "        'orientation_reward': orientation_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_components\n",
      "```\n",
      "\n",
      "Task 4 Name\n",
      "Minimize Goal Distance\n",
      "\n",
      "Task 4 Description\n",
      "The task \"Minimize Goal Distance\" focuses on reducing the distance between the current position of the ant's torso and the goal position. The reward function encourages the ant to move towards the goal as efficiently as possible, effectively solving the navigation problem in the maze.\n",
      "\n",
      "```python\n",
      "def minimize_goal_distance_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Calculate the current distance to the goal\n",
      "    current_goal_distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Reward for being close to the goal.\n",
      "    # A weight factor can be used to scale the importance of minimizing the distance.\n",
      "    goal_distance_weight = 1.0\n",
      "    goal_distance_reward = -goal_distance_weight * current_goal_distance\n",
      "    \n",
      "    # Total reward is the negative of the goal distance to encourage minimization\n",
      "    total_reward = goal_distance_reward\n",
      "\n",
      "    # Dictionary to store individual reward components\n",
      "    reward_components = {\n",
      "        'goal_distance_reward': goal_distance_reward\n",
      "    }\n",
      "    \n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 3 Name\n",
      "Goal Orientation\n",
      "\n",
      "Task 3 Description\n",
      "Minimize the distance between the ant's torso and the goal position within the maze environment, based on the provided observation space.\n",
      "\n",
      "```python\n",
      "def goal_orientation_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Weights for reward components\n",
      "    goal_distance_weight = 1.0  # You can adjust weights to balance reward components\n",
      "\n",
      "    # Calculate the distance to the goal from the torso position\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Utilize negative L2 norm to incentivize minimizing the distance to the goal\n",
      "    # Closer to zero is better, so we'll take the negative to encourage smaller values\n",
      "    reward_distance_to_goal = -goal_distance_weight * np.linalg.norm(distance_to_goal)\n",
      "\n",
      "    # Total reward is a combination of components\n",
      "    reward = reward_distance_to_goal\n",
      "    \n",
      "    # Include individual reward components for debugging and analysis\n",
      "    reward_components = {\n",
      "        'distance_to_goal': reward_distance_to_goal\n",
      "    }\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 4 Name\n",
      "Navigation and Turning\n",
      "\n",
      "Task 4 Description\n",
      "Maximize the ant robot's forward position (x-coordinate of torso_coordinate) while adjusting the torso's orientation to face multiple random goals (goal_pos), without actually reaching the goals.\n",
      "\n",
      "```python\n",
      "def navigation_turning_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Collect necessary observations\n",
      "    x_coordinate = self.torso_coordinate(ant_obs)[0]\n",
      "    goal_direction = self.goal_pos() - self.torso_coordinate(ant_obs)[:2]\n",
      "    goal_angle = np.arctan2(goal_direction[1], goal_direction[0])\n",
      "\n",
      "    # Components of the reward\n",
      "    forward_reward_weight = 1.0\n",
      "    turning_reward_weight = 0.5\n",
      "\n",
      "    # Calculate the reward for forward movement along the x-axis\n",
      "    forward_reward = np.tanh(x_coordinate)\n",
      "    # Apply an L2 norm to the difference in angle to face the goal\n",
      "    current_orientation = self.torso_orientation(ant_obs)\n",
      "    current_heading_angle = np.arctan2(current_orientation[1], current_orientation[0])\n",
      "    turning_reward = -np.tanh(np.linalg.norm(current_heading_angle - goal_angle))\n",
      "\n",
      "    # Aggregate the reward with their respective weights\n",
      "    reward = (forward_reward_weight * forward_reward) + (turning_reward_weight * turning_reward)\n",
      "    reward_components = {\n",
      "        \"forward_reward\": forward_reward,\n",
      "        \"turning_reward\": turning_reward\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "This reward function drives the ant to increase its x-coordinate, which means moving forward in the maze, while also turning to face the intended random goal directions. The use of the hyperbolic tangent function helps keep rewards bounded within a sensible range for each respective task, and weights allow the user to balance how much each component contributes to the total reward.\n",
      "Task 4 Name\n",
      "Navigation and Turning\n",
      "\n",
      "Task 4 Description\n",
      "The goal of the Navigation and Turning task is to maximize the movement of the ant's torso (forward progression) while also changing the torso orientation to face towards various goal positions randomly placed in the environment. The ant does not need to reach the goals but must turn to face them, promoting the behavior of navigation and turning without focusing on goal completion.\n",
      "\n",
      "```python\n",
      "def navigation_and_turning_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Extract relevant information from observations\n",
      "    torso_pos = self.torso_coordinate(ant_obs)\n",
      "    torso_orient = self.torso_orientation(ant_obs)\n",
      "    goal_position = self.goal_pos()\n",
      "    \n",
      "    # Components of the reward signal\n",
      "    progress_weight = 1.0  # Weighting for forward progression\n",
      "    orientation_weight = 0.5  # Weighting for torso orientation adjustment\n",
      "    \n",
      "    # Reward for moving forward, penalizing movements that are not in the x-direction\n",
      "    progress_reward = np.tanh(torso_pos[0]) - 0.5 * np.linalg.norm(torso_pos[1:])\n",
      "    \n",
      "    # Compute the vector pointing toward the goal from the current position\n",
      "    direction_to_goal = goal_position[:2] - torso_pos[:2]\n",
      "    # Normalize vectors for computing the dot product\n",
      "    forward_vector = np.array([1.0, 0.0])  # Assuming that the ant's forward direction aligns with the x-axis\n",
      "    direction_to_goal /= (np.linalg.norm(direction_to_goal) + 1e-8)\n",
      "    \n",
      "    # Reward for torso orientation aligns with vector to the goal position\n",
      "    orientation_reward = np.dot(direction_to_goal, forward_vector)\n",
      "    # Use an arccos to evaluate how well the torso is oriented toward the goal (0 means facing)\n",
      "    orientation_reward = np.arccos(orientation_reward) / np.pi   # Normalized between [0, 1]\n",
      "    orientation_reward = np.tanh(1 - orientation_reward)          # Encourage to face the goal\n",
      "    \n",
      "    # Compute the total reward with the associated weights for each component\n",
      "    reward = progress_weight * progress_reward + orientation_weight * orientation_reward\n",
      "    \n",
      "    # Create a dictionary to store the individual reward components\n",
      "    reward_components = {\n",
      "        'progress_reward': progress_weight * progress_reward,\n",
      "        'orientation_reward': orientation_weight * orientation_reward,\n",
      "    }\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "\n",
      "Please note that the forward direction of the ant is assumed to align with the x-axis in the world coordinate system for this calculation, and any deviation from the forward movement reduces the progress reward. The orientation reward encourages the ant to turn such that its x-axis is aligned with the vector pointing to the goal.\n",
      "Task 4 Name\n",
      "Navigation and Turning\n",
      "\n",
      "Task 4 Description\n",
      "Maximize torso_coordinate while changing torso_orientation to face multiple random goal_pos without actually reaching them. This task focuses on the ant robot's ability to orient and navigate towards multiple random goals in its environment. It requires the ant to be able to turn towards a given goal without necessarily reaching it.\n",
      "\n",
      "```python\n",
      "def navigation_and_turning_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Constants/Weights for reward components\n",
      "    coordinate_weight = 1.0\n",
      "    orientation_weight = 10.0\n",
      "    \n",
      "    # Getting components from the environment\n",
      "    xyz_coordinate = self.torso_coordinate(ant_obs)          # Torso position (x, y, z coordinates)\n",
      "    orientation = self.torso_orientation(ant_obs)            # Torso orientation (quaternions)\n",
      "    goal_pos = self.goal_pos()                               # Position of the goal\n",
      "    flat_goal_pos = goal_pos[:2]                             # Flattening to 2D for x and y coordinates\n",
      "\n",
      "    # Calculating distance to the goal\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Component 1: Maximizing x and y coordinates (Navigation)\n",
      "    navigation_reward = np.tanh(np.linalg.norm(xyz_coordinate[:2]))     # Taking only x and y for navigation\n",
      "    navigation_reward *= coordinate_weight                             # Apply weighting\n",
      "    \n",
      "    # Component 2: Reward for changing orientation to face the goal (Turning)\n",
      "    # This uses the difference in yaw between the ant's current orientation and the target goal.\n",
      "    # Note: This is a simplification for the example. Proper orientation difference in 3D space would involve quaternion math.\n",
      "    desired_yaw_to_goal = np.arctan2(flat_goal_pos[1] - xyz_coordinate[1], flat_goal_pos[0] - xyz_coordinate[0])\n",
      "    current_yaw = np.arctan2(orientation[2], orientation[3])  # Assuming orientation[3] represents the forward direction (cosine component)\n",
      "    yaw_diff = np.linalg.norm(desired_yaw_to_goal - current_yaw) \n",
      "    \n",
      "    # Apply tangens hyperbolicus to reward signal for orientation difference\n",
      "    orientation_reward = -np.tanh(yaw_diff)     # We use negative because we want to minimize the difference\n",
      "    orientation_reward *= orientation_weight    # Apply weighting\n",
      "    \n",
      "    # Reward is a combination of both navigation and orientation adjustments\n",
      "    reward = navigation_reward + orientation_reward\n",
      "    \n",
      "    # Breakdown of reward components for analysis\n",
      "    reward_components = {\n",
      "        'navigation_reward': navigation_reward,\n",
      "        'orientation_reward': orientation_reward\n",
      "    }\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 4 Name\n",
      "Navigation and Turning\n",
      "\n",
      "Task 4 Description\n",
      "The reward function for \"Navigation and Turning\" should encourage the ant to navigate through the environment towards various goals without expecting it to actually reach the goals. The ant should adjust its torso_orientation to face the direction of the next goal position.\n",
      "\n",
      "```python\n",
      "def navigation_and_turning_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define the weights for different components of the reward\n",
      "    coordinate_weight = 0.5  # Weight for the torso coordinate movement\n",
      "    orientation_weight = 0.3  # Weight for the torso orientation\n",
      "    angular_velocity_weight = 0.2  # Weight to minimize angular velocity\n",
      "    \n",
      "    # Extract necessary observation components\n",
      "    torso_coordinate_xz = self.torso_coordinate(ant_obs)[:2]  # We care mainly about x and z axis\n",
      "    torso_orientation = self.torso_orientation(ant_obs)\n",
      "    torso_angular_velocity = self.torso_angular_velocity(ant_obs)\n",
      "    goal_pos = self.goal_pos()[:2]  # Consider only the x and z components of the goal position\n",
      "\n",
      "    # Compute the direction vector from the ant to the goal\n",
      "    direction_to_goal = goal_pos - torso_coordinate_xz\n",
      "    \n",
      "    # Normalize the direction vector\n",
      "    if np.linalg.norm(direction_to_goal) > 0:\n",
      "        direction_to_goal /= np.linalg.norm(direction_to_goal)\n",
      "\n",
      "    # Compute the reward for moving closer to the goal using xz coordinates\n",
      "    coordinate_reward = np.tanh(np.linalg.norm(torso_coordinate_xz - goal_pos))\n",
      "\n",
      "    # Compute the reward for facing towards the goal (using the dot product between orientation and direction vector)\n",
      "    orientation_reward = np.dot(torso_orientation[:2], direction_to_goal)\n",
      "\n",
      "    # Minimize angular velocity to encourage smooth turning\n",
      "    angular_velocity_penalty = -np.tanh(np.linalg.norm(torso_angular_velocity))\n",
      "\n",
      "    # Calculate the total weighted reward\n",
      "    reward = (\n",
      "        coordinate_weight * coordinate_reward +\n",
      "        orientation_weight * orientation_reward +\n",
      "        angular_velocity_weight * angular_velocity_penalty\n",
      "    )\n",
      "\n",
      "    # Store the individual reward components for analysis and debugging\n",
      "    reward_components = {\n",
      "        'coordinate_reward': coordinate_reward,\n",
      "        'orientation_reward': orientation_reward,\n",
      "        'angular_velocity_penalty': angular_velocity_penalty\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 4 Name\n",
      "Navigation and Turning\n",
      "\n",
      "Task 4 Description\n",
      "The objective is for the ant to adjust its torso_orientation to face multiple random goal positions (goal_pos) without the need to physically reach the goals. This task focuses on the ant's ability to align itself with the desired direction, a crucial capability for navigation, especially in a maze setting.\n",
      "\n",
      "```python\n",
      "def navigation_and_turning_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Constants\n",
      "    orientation_weight = 2.0\n",
      "    coordinate_weight = 1.0\n",
      "    forward_weight = 1.0\n",
      "    distance_weight = -0.5  # Penalize for goal distance to avoid reaching the target\n",
      "    turning_weight = 1.0\n",
      "\n",
      "    # Component Rewards\n",
      "    reward_components = {}\n",
      "\n",
      "    # Get information from observations\n",
      "    torso_coordinate = self.torso_coordinate(ant_obs)\n",
      "    torso_orientation = self.torso_orientation(ant_obs)\n",
      "    goal_position = self.goal_pos()\n",
      "    torso_velocity = self.torso_velocity(ant_obs)\n",
      "\n",
      "    # Coordinate component (encourage maximizing torso's X-coordinate for forward movements)\n",
      "    coordinate_reward = np.tanh(torso_coordinate[0])  # Mostly interested in X-coordinate for forward movement\n",
      "    reward_components['coordinate'] = coordinate_reward * coordinate_weight\n",
      "\n",
      "    # Velocity component (encourage movement)\n",
      "    velocity_reward = np.linalg.norm(torso_velocity)\n",
      "    forward_reward = np.tanh(velocity_reward)  # Encourage the ant moving forward\n",
      "    reward_components['forward'] = forward_reward * forward_weight\n",
      "\n",
      "    # Distance to goal component (but do not actually need to reach the goal)\n",
      "    distance_to_goal = self.goal_distance(ant_obs)\n",
      "    distance_reward = -np.tanh(distance_to_goal)  # Discourage getting too close to the goal\n",
      "    reward_components['distance'] = distance_reward * distance_weight\n",
      "\n",
      "    # Orientation component (encourage ant to turn and face the goal)\n",
      "    # For simplicity, we'll turn the ant to face the direction of the goal along the horizontal plane\n",
      "    # by comparing the vector to the goal with the ant's forward vector (which corresponds to its orientation)\n",
      "    goal_direction = (goal_position - torso_coordinate[:2]) / np.linalg.norm(goal_position - torso_coordinate[:2])\n",
      "    ant_forward_vector = np.array([1, 0])  # Assuming ant's forward corresponds to x-axis\n",
      "    direction_dot_product = np.dot(goal_direction, ant_forward_vector)\n",
      "    orientation_reward = np.tanh(direction_dot_product)  # Reward aligned orientation with goal direction\n",
      "    reward_components['orientation'] = orientation_reward * orientation_weight\n",
      "\n",
      "    # Turning speed component (encourage ant to quickly turn towards the goal)\n",
      "    # Assuming we want to monitor the angular velocity about the z-axis for turning\n",
      "    turning_speed = torso_orientation[5]  # Taking the z-component for turning speed\n",
      "    turning_reward = np.tanh(turning_speed)  # Encourage faster turning\n",
      "    reward_components['turning'] = turning_reward * turning_weight\n",
      "\n",
      "    # Calculate total reward\n",
      "    total_reward = (\n",
      "        reward_components['coordinate'] +\n",
      "        reward_components['forward'] +\n",
      "        reward_components['distance'] +\n",
      "        reward_components['orientation'] +\n",
      "        reward_components['turning']\n",
      "    )\n",
      "\n",
      "    return total_reward, reward_components\n",
      "```\n",
      "Task 5 Name\n",
      "Original Task\n",
      "\n",
      "Task 5 Description\n",
      "The goal of the reward function for the original task in the AntMazeEnv environment is to design a reward function that encourages the ant to maintain the goal_distance at a value of 0.45. The reward should penalize deviations from this desired distance.\n",
      "\n",
      "```python\n",
      "def original_task_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Initialize reward dictionary.\n",
      "    reward_components: Dict[str, np.float64] = {}\n",
      "    \n",
      "    # Define the weight for the goal_distance component.\n",
      "    goal_distance_weight: np.float64 = 1.0\n",
      "    \n",
      "    # Calculate the distance to the goal position.\n",
      "    goal_distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Use L2 norm to create a reward component based on the goal_distance.\n",
      "    # The further from 0.45, the more negative the reward will be.\n",
      "    goal_distance_reward: np.float64 = -goal_distance_weight * np.linalg.norm(goal_distance - 0.45)\n",
      "    \n",
      "    # Store the reward components in the dictionary.\n",
      "    reward_components[\"goal_distance_reward\"] = goal_distance_reward\n",
      "    \n",
      "    # Compute the total reward.\n",
      "    reward = np.sum(list(reward_components.values()))\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 5 Name\n",
      "Original Task\n",
      "\n",
      "Task 5 Description\n",
      "The reward function should encourage the ant robot to maintain a goal distance of 0.45 units from the target position within the closed maze.\n",
      "\n",
      "```python\n",
      "def original_task_reward(self, ant_obs: np.ndarray) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Constants for reward terms\n",
      "    goal_distance_target = 0.45\n",
      "    distance_weight = 1.0\n",
      "    \n",
      "    # Compute the distance from the goal\n",
      "    distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Reward component for maintaining the desired goal distance (0.45)\n",
      "    distance_reward = -distance_weight * np.linalg.norm(distance - goal_distance_target)\n",
      "    \n",
      "    # Total reward is the sum of individual components\n",
      "    reward = distance_reward\n",
      "    \n",
      "    # Populate reward components as a dictionary for debugging or analysis\n",
      "    reward_components = {\n",
      "        'distance_reward': distance_reward,\n",
      "    }\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 5 Name\n",
      "Original Task\n",
      "\n",
      "Task 5 Description\n",
      "The objective is to maintain `goal_distance` as close as possible to the target value of `0.45`, where `goal_distance` is the Euclidean distance between the ant's torso and the goal position in the maze.\n",
      "\n",
      "```python\n",
      "def original_task_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define the target value for goal_distance\n",
      "    target_goal_distance = 0.45\n",
      "    \n",
      "    # Retrieve the current goal distance\n",
      "    current_goal_distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Define a weighting parameter for the goal distance reward component\n",
      "    goal_distance_weight = 1.0\n",
      "    \n",
      "    # Compute the L2 norm difference between current and target goal distances\n",
      "    goal_distance_error = np.linalg.norm(current_goal_distance - target_goal_distance)\n",
      "  \n",
      "    # Convert the error into a reward, where a smaller distance error gives a higher reward\n",
      "    # Since the function should return a positive reward for closer distances, we use a negative sign.\n",
      "    goal_distance_reward = -goal_distance_weight * goal_distance_error\n",
      "\n",
      "    # Total reward is just the goal distance reward in this case\n",
      "    reward = goal_distance_reward\n",
      "    \n",
      "    # Create a dictionary of individual reward components\n",
      "    reward_components = {\n",
      "        \"goal_distance_reward\": goal_distance_reward\n",
      "    }\n",
      "    \n",
      "    return reward, reward_components\n",
      "```\n",
      "Task 5 Name\n",
      "Original Task\n",
      "\n",
      "Task 5 Description\n",
      "The goal is to maintain the position of the ant robot such that the distance to the goal remains constant at a value of 0.45 units away from the target position. This involves the ant performing actions that precisely position it at a set distance from the desired goal.\n",
      "\n",
      "```python\n",
      "def original_task_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define the specific goal distance that we want the ant to maintain\n",
      "    desired_goal_distance = 0.45\n",
      "    \n",
      "    # Calculate the current distance to the goal\n",
      "    current_goal_distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # The reward is the negative L2 norm between the current and desired goal distance,\n",
      "    # this enforces the agent to minimize the difference\n",
      "    distance_error = np.linalg.norm(current_goal_distance - desired_goal_distance)\n",
      "    \n",
      "    # We want the distance_error to be zero, so we will reward the agent more the smaller the error is\n",
      "    # A tanh transformation is used to smoothly penalize the agent as it deviates from the desired distance.\n",
      "    # This ensures that the reward is bounded and has nice properties for optimization.\n",
      "    distance_reward_component = -np.tanh(distance_error)\n",
      "    \n",
      "    # Define a weighting parameter for the distance error component\n",
      "    distance_error_weight = 1.0\n",
      "    \n",
      "    # Calculate the total reward by applying the weighting to the distance reward component\n",
      "    reward = distance_error_weight * distance_reward_component\n",
      "    \n",
      "    # Store reward information in a dictionary for debugging and analysis\n",
      "    reward_info = {\n",
      "        \"distance_reward_component\": distance_reward_component,\n",
      "        \"distance_error_weight\": distance_error_weight\n",
      "    }\n",
      "    \n",
      "    return reward, reward_info\n",
      "```\n",
      "Task 5 Name\n",
      "Original Task\n",
      "\n",
      "Task 5 Description\n",
      "The reward function encourages the ant to minimize the distance between its current position and the goal position to approximately 0.45 units.\n",
      "\n",
      "```python\n",
      "def original_task_reward(self, ant_obs) -> Tuple[np.float64, Dict[str, np.float64]]:\n",
      "    # Define the goal distance value to be achieved\n",
      "    goal_distance_value = 0.45\n",
      "    \n",
      "    # Obtain the current distance to the goal from the ant's position\n",
      "    current_goal_distance = self.goal_distance(ant_obs)\n",
      "\n",
      "    # Calculate the proximity of the current goal distance to the desired goal distance\n",
      "    # Use an L2 norm to penalize the abs difference between current and desired goal distance\n",
      "    distance_to_goal_reward_component = -np.linalg.norm(current_goal_distance - goal_distance_value)\n",
      "    \n",
      "    # Weighting parameter for the distance to goal reward component\n",
      "    distance_to_goal_weight = 1.0\n",
      "\n",
      "    # Compute the total reward by combining the reward components\n",
      "    reward = distance_to_goal_weight * distance_to_goal_reward_component\n",
      "    \n",
      "    # Create a dictionary of each individual reward component\n",
      "    reward_components = {\n",
      "        'distance_to_goal_reward': distance_to_goal_reward_component,\n",
      "    }\n",
      "\n",
      "    return reward, reward_components\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "reward_system = file_to_string('./gpt/prompt/AntMaze_UMaze/env_specific_reward_system.txt')\n",
    "reward_user = file_to_string('./gpt/prompt/AntMaze_UMaze/env_specific_reward_user.txt')\n",
    "\n",
    "for task_idx in range(int(task_length)):\n",
    "    curriculum_answer = tasks[2*task_idx] + tasks[2*task_idx+1]\n",
    "    reward_user_itr = reward_user + \"\\n\" + curriculum_answer\n",
    "    for i in range(5):\n",
    "        reward_functions = gpt_interaction(reward_system, reward_user_itr) \n",
    "\n",
    "        save_string_to_file(save_path=f\"./gpt/prompt/AntMaze_UMaze/env_specific_reward_task_{task_idx}_sample_{i}.md\", string_file=reward_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find statistics that is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required variables: [x_velocity_of_the_torso, healthy_reward, control_cost, z_coordinate_of_the_torso, angle_of_the_torso]\n",
      "[x_velocity_of_the_torso]: [Maximized]\n",
      "[healthy_reward]: [Maximized]\n",
      "[control_cost]: [Minimized]\n",
      "[z_coordinate_of_the_torso]: [Close to [0.7]]\n",
      "[angle_of_the_torso]: [Close to [0.0]]\n",
      "\n",
      "Reason:\n",
      "The x velocity of the torso should be maximized in order to achieve the highest forward speed.\n",
      "\n",
      "The healthy reward should be maximized because it indicates that the hopper is maintaining its height above 0.7 and keeping the torso angle within the desired range, which corresponds to being upright.\n",
      "\n",
      "Control cost should be minimized to ensure that the actions taken by the hopper, in terms of torques applied to hinges, are not excessively large, leading to more efficient and controlled hops.\n",
      "\n",
      "The z coordinate of the torso should be close to 0.7, as it directly correlates with the constraint mentioned in the healthy reward description, implying the hopper is airborne or maintaining sufficient height while hopping.\n",
      "\n",
      "The angle of the torso should be close to 0.0 radians indicating that the torso is upright, which is critical for maintaining balance and achieving forward momentum without falling.\n"
     ]
    }
   ],
   "source": [
    "system = file_to_string('./gpt/prompt/test/statistics_system.txt')\n",
    "user = file_to_string('./gpt/prompt/test/statistics_user.txt')\n",
    "\n",
    "statistics_answer = gpt_interaction(system, user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_healthy(observation):\n",
    "    z, angle = observation[1:3]\n",
    "    state = observation[2:]\n",
    "\n",
    "    min_state, max_state = (-100, 100)\n",
    "    min_z, max_z = (0.7, np.inf)\n",
    "    min_angle, max_angle = (-0.2, 0.2)\n",
    "\n",
    "    healthy_state = np.all(np.logical_and(min_state < state, state < max_state))\n",
    "    healthy_z = min_z < z < max_z\n",
    "    healthy_angle = min_angle < angle < max_angle\n",
    "\n",
    "    is_healthy = all((healthy_state, healthy_z, healthy_angle))\n",
    "\n",
    "    return int(is_healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/stand_still_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "# Get average of the is_healthy\n",
    "healthy = np.array([is_healthy(observation) for observation in obs])\n",
    "healthy_avg = np.mean(healthy)\n",
    "\n",
    "stand_still_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std) + \"\\nAverage is healthy: \" + str(healthy_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.loadtxt('./gpt/prompt/test/simple_hopping_observation.txt', delimiter=',')\n",
    "\n",
    "# Get average and std of the x position\n",
    "x_pos = obs[:, 0]\n",
    "x_pos_avg = np.mean(x_pos)\n",
    "x_pos_std = np.std(x_pos)\n",
    "\n",
    "# Get average and std of the z position\n",
    "z_pos = obs[:, 1]\n",
    "z_pos_avg = np.mean(z_pos)\n",
    "z_pos_std = np.std(z_pos)\n",
    "\n",
    "# Get average and std of x velocity\n",
    "x_pos_next = obs[1:, 0]\n",
    "x_vel = (x_pos_next - x_pos[:-1]) / 0.008\n",
    "x_vel_avg = np.mean(x_vel)\n",
    "x_vel_std = np.std(x_vel)\n",
    "\n",
    "# Get average and std of z velocity\n",
    "z_pos_next = obs[1:, 1]\n",
    "z_vel = (z_pos_next - z_pos[:-1]) / 0.008\n",
    "z_vel_avg = np.mean(z_vel)\n",
    "z_vel_std = np.std(z_vel)\n",
    "\n",
    "# Get average of the is_healthy\n",
    "healthy = np.array([is_healthy(observation) for observation in obs])\n",
    "healthy_avg = np.mean(healthy)\n",
    "\n",
    "simple_hopping_info = \"\\nAverage x position: \" + str(x_pos_avg) + \"\\nStandard deviation of x position: \" + str(x_pos_std) + \"\\nAverage z position: \" + str(z_pos_avg) + \"\\nStandard deviation of z position: \" + str(z_pos_std) + \"\\nAverage x velocity: \" + str(x_vel_avg) + \"\\nStandard deviation of x velocity: \" + str(x_vel_std) + \"\\nAverage z velocity: \" + str(z_vel_avg) + \"\\nStandard deviation of z velocity: \" + str(z_vel_std) + \"\\nAverage is healthy: \" + str(healthy_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision: Agent 1\n",
      "Reason: \n",
      "\n",
      "Agent 1 exhibits no horizontal movement as the average and standard deviation of x position and velocity are both zero, which perfectly adheres to the task requirement of not moving forward or backward. Furthermore, Agent 1 has a high average z position, more consistent hopping height (lower standard deviation of z position), and maintains the healthy status throughout the trajectory. While the standard deviation of z velocity is non-zero, which indicates variability in the hopping speed, there's no displacement in the x-direction, so the trajectory meets the task requirements effectively. In contrast, Agent 2 has shown drift in the x-direction as indicated by the non-zero average x position and significant standard deviation of x position, suggesting it's not strictly hopping in place. The high standard deviation in x velocity for Agent 2 also demonstrates inconsistency in maintaining a stationary hop. Even though Agent 2 has a higher average z position, the deviation from the required task in the x-direction makes it less suitable than Agent 1.\n"
     ]
    }
   ],
   "source": [
    "assistant = statistics_answer\n",
    "\n",
    "user_2 = file_to_string('./gpt/prompt/test/statistics_user_2.txt')\n",
    "user_2 = user_2 + \"\\nAgent 1\" + stand_still_info + \"\\nAgent 2\" + simple_hopping_info\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\", # gpt-4-1106-preview, gpt-4-0613, gpt-4-32k, gpt-3.5-turbo-1106\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": user},\n",
    "    {\"role\": \"assistant\", \"content\": assistant},\n",
    "    {\"role\": \"user\", \"content\": user_2},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
