{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt.curriculum_api_chain import CurriculumAPI\n",
    "# from train_ant import Curriculum_Module\n",
    "\n",
    "api = CurriculumAPI(\"AntMaze_UMaze\", \"./gpt/prompt/\", \"./logs/AntMaze_UMaze_0/\")\n",
    "# module = Curriculum_Module(\"AntMaze_UMaze\", \"./environments/Curriculum/envs/AntMaze_UMaze.py\", \"./logs/AntMaze_UMaze_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1\n",
      "Name: Basic Locomotion\n",
      "Description: Maximize torso_velocity\n",
      "Reason: The agent needs to first learn how to generate forward movement. Focusing on maximizing torso_velocity ensures the agent learns to coordinate its legs for movement, a foundational skill before navigating to specific targets.\n",
      "\n",
      "Task 2\n",
      "Name: Orientation Control\n",
      "Description: Minimize torso_angular_velocity\n",
      "Reason: After learning to move, the agent should learn how to maintain a stable orientation. By minimizing torso_angular_velocity, the agent practices maintaining a stable orientation while moving, crucial for efficiently navigating towards goals without unnecessary rotations.\n",
      "\n",
      "Task 3\n",
      "Name: Directed Movement\n",
      "Description: Minimize goal_distance with a denser reward being given as the goal_distance decreases\n",
      "Reason: This task transitions the agent from basic movement and stability to purposeful navigation. Minimizing goal_distance with a dense reward structure teaches the agent to move towards the goal effectively, setting the stage for goal-oriented navigation in a more complex environment like a maze.\n",
      "\n",
      "Task 4\n",
      "Name: Original task\n",
      "Description: Give reward if the distance between torso xy position and goal xy position is less than 0.45. Return 0 otherwise.\n",
      "Reason: This final task integrates all the skills the agent has developed: locomotion, stability, and directed movement, within the context of the original goal-reaching in a maze environment. The sparse reward signals successful completion of the maze navigation challenge.\n",
      "[{'Task': '1', 'Name': 'Basic Locomotion', 'Description': 'Maximize torso_velocity', 'Reason': 'The agent needs to first learn how to generate forward movement. Focusing on maximizing torso_velocity ensures the agent learns to coordinate its legs for movement, a foundational skill before navigating to specific targets.'}, {'Task': '2', 'Name': 'Orientation Control', 'Description': 'Minimize torso_angular_velocity', 'Reason': 'After learning to move, the agent should learn how to maintain a stable orientation. By minimizing torso_angular_velocity, the agent practices maintaining a stable orientation while moving, crucial for efficiently navigating towards goals without unnecessary rotations.'}, {'Task': '3', 'Name': 'Directed Movement', 'Description': 'Minimize goal_distance with a denser reward being given as the goal_distance decreases', 'Reason': 'This task transitions the agent from basic movement and stability to purposeful navigation. Minimizing goal_distance with a dense reward structure teaches the agent to move towards the goal effectively, setting the stage for goal-oriented navigation in a more complex environment like a maze.'}, {'Task': '4', 'Name': 'Original task', 'Description': 'Give reward if the distance between torso xy position and goal xy position is less than 0.45. Return 0 otherwise.', 'Reason': 'This final task integrates all the skills the agent has developed: locomotion, stability, and directed movement, within the context of the original goal-reaching in a maze environment. The sparse reward signals successful completion of the maze navigation challenge.'}]\n"
     ]
    }
   ],
   "source": [
    "task_details = api.generate_curriculum()\n",
    "print(task_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"```python\n",
      "def compute_reward_curriculum(self):\n",
      "    # Gather necessary observation data\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    velocity_weight = 2.0  # Higher weight for current task\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = np.linalg.norm(torso_vel)  # We care about the magnitude of velocity for locomotion\n",
      "\n",
      "    # Reward for maximizing forward movement\n",
      "    reward_forward = np.tanh(forward_velocity) * velocity_weight\n",
      "\n",
      "    # Total reward is simply the reward for forward movement in this task as our main focus is on basic locomotion\n",
      "    reward = reward_forward\n",
      "\n",
      "    # Reward breakdown for potential debugging and analysis\n",
      "    reward_dict = {\n",
      "        'reward_forward': reward_forward,\n",
      "    }\n",
      "\n",
      "    return reward, reward_dict\n",
      "```\"\n",
      "Extracted Code Block:\n",
      " def compute_reward_curriculum(self):\n",
      "    # Gather necessary observation data\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    velocity_weight = 2.0  # Higher weight for current task\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = np.linalg.norm(torso_vel)  # We care about the magnitude of velocity for locomotion\n",
      "\n",
      "    # Reward for maximizing forward movement\n",
      "    reward_forward = np.tanh(forward_velocity) * velocity_weight\n",
      "\n",
      "    # Total reward is simply the reward for forward movement in this task as our main focus is on basic locomotion\n",
      "    reward = reward_forward\n",
      "\n",
      "    # Reward breakdown for potential debugging and analysis\n",
      "    reward_dict = {\n",
      "        'reward_forward': reward_forward,\n",
      "    }\n",
      "\n",
      "    return reward, reward_dict\n"
     ]
    }
   ],
   "source": [
    "code = api.generate_rewards(0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def compute_reward_curriculum(self):\n",
      "    # Gather necessary observation data\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    \n",
      "    # Weights for each component of the reward function, balancing previous and current tasks\n",
      "    velocity_weight = 1.0  # Still important but less than current task\n",
      "    angular_velocity_weight = 2.0  # Higher weight for current task to emphasize learning orientation control\n",
      "    \n",
      "    # Compute forward velocity for the basic locomotion task\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = np.linalg.norm(torso_vel)  # Magnitude of velocity for locomotion\n",
      "    \n",
      "    # Compute angular velocity for the orientation control task\n",
      "    torso_angular_vel = self.torso_angular_velocity(ant_obs)\n",
      "    angular_velocity_magnitude = np.linalg.norm(torso_angular_vel)  # Magnitude of angular velocity for orientation control\n",
      "    \n",
      "    # Reward components\n",
      "    reward_forward = np.tanh(forward_velocity) * velocity_weight  # Reward for forward movement from previous task\n",
      "    reward_orientation_control = -np.tanh(angular_velocity_magnitude) * angular_velocity_weight  # Penalty for high angular velocity, encouraging orientation stability\n",
      "    \n",
      "    # Total reward combines both aspects, balancing ongoing development of basic locomotion and orientation control\n",
      "    reward = reward_forward + reward_orientation_control\n",
      "    \n",
      "    # Reward breakdown for potential debugging and analysis\n",
      "    reward_dict = {\n",
      "        'reward_forward': reward_forward,\n",
      "        'reward_orientation_control': reward_orientation_control,\n",
      "    }\n",
      "\n",
      "    return reward, reward_dict\n",
      "```\n",
      "Extracted Code Block:\n",
      " def compute_reward_curriculum(self):\n",
      "    # Gather necessary observation data\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    \n",
      "    # Weights for each component of the reward function, balancing previous and current tasks\n",
      "    velocity_weight = 1.0  # Still important but less than current task\n",
      "    angular_velocity_weight = 2.0  # Higher weight for current task to emphasize learning orientation control\n",
      "    \n",
      "    # Compute forward velocity for the basic locomotion task\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = np.linalg.norm(torso_vel)  # Magnitude of velocity for locomotion\n",
      "    \n",
      "    # Compute angular velocity for the orientation control task\n",
      "    torso_angular_vel = self.torso_angular_velocity(ant_obs)\n",
      "    angular_velocity_magnitude = np.linalg.norm(torso_angular_vel)  # Magnitude of angular velocity for orientation control\n",
      "    \n",
      "    # Reward components\n",
      "    reward_forward = np.tanh(forward_velocity) * velocity_weight  # Reward for forward movement from previous task\n",
      "    reward_orientation_control = -np.tanh(angular_velocity_magnitude) * angular_velocity_weight  # Penalty for high angular velocity, encouraging orientation stability\n",
      "    \n",
      "    # Total reward combines both aspects, balancing ongoing development of basic locomotion and orientation control\n",
      "    reward = reward_forward + reward_orientation_control\n",
      "    \n",
      "    # Reward breakdown for potential debugging and analysis\n",
      "    reward_dict = {\n",
      "        'reward_forward': reward_forward,\n",
      "        'reward_orientation_control': reward_orientation_control,\n",
      "    }\n",
      "\n",
      "    return reward, reward_dict\n"
     ]
    }
   ],
   "source": [
    "code_list = [code]\n",
    "code = api.generate_rewards(1, code_list)\n",
    "code_list.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def compute_reward_curriculum(self):\n",
      "    # Gather necessary observation data\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    \n",
      "    # Weights for each component of the reward function, balancing previous and current tasks\n",
      "    velocity_weight = 0.5  # Previous task, still relevant but less weight\n",
      "    angular_velocity_weight = 0.5  # Previous task, orientation control\n",
      "    goal_distance_weight = 2.0  # Current task, emphasize minimizing distance to goal\n",
      "    \n",
      "    # Compute forward velocity for the basic locomotion task\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = np.linalg.norm(torso_vel)  # Magnitude of velocity for locomotion\n",
      "    \n",
      "    # Compute angular velocity for the orientation control task\n",
      "    torso_angular_vel = self.torso_angular_velocity(ant_obs)\n",
      "    angular_velocity_magnitude = np.linalg.norm(torso_angular_vel)  # Magnitude for orientation control\n",
      "    \n",
      "    # Compute goal distance for the directed movement task\n",
      "    goal_distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Reward components\n",
      "    reward_forward = np.tanh(forward_velocity) * velocity_weight  # Reward for forward movement from basic locomotion\n",
      "    reward_orientation_control = -np.tanh(angular_velocity_magnitude) * angular_velocity_weight  # Penalty for high angular velocity, encouraging orientation stability\n",
      "    reward_goal_distance = -np.tanh(goal_distance) * goal_distance_weight  # Higher penalty as the goal distance increases, encouraging movement towards the goal\n",
      "    \n",
      "    # Total reward combines all aspects, factoring in basic locomotion, orientation control, and directed movement towards goal\n",
      "    reward = reward_forward + reward_orientation_control + reward_goal_distance\n",
      "    \n",
      "    # Reward breakdown for potential debugging and analysis\n",
      "    reward_dict = {\n",
      "        'reward_forward': reward_forward,\n",
      "        'reward_orientation_control': reward_orientation_control,\n",
      "        'reward_goal_distance': reward_goal_distance,\n",
      "    }\n",
      "\n",
      "    return reward, reward_dict\n",
      "```\n",
      "Extracted Code Block:\n",
      " def compute_reward_curriculum(self):\n",
      "    # Gather necessary observation data\n",
      "    ant_obs = self.get_ant_obs()\n",
      "    \n",
      "    # Weights for each component of the reward function, balancing previous and current tasks\n",
      "    velocity_weight = 0.5  # Previous task, still relevant but less weight\n",
      "    angular_velocity_weight = 0.5  # Previous task, orientation control\n",
      "    goal_distance_weight = 2.0  # Current task, emphasize minimizing distance to goal\n",
      "    \n",
      "    # Compute forward velocity for the basic locomotion task\n",
      "    torso_vel = self.torso_velocity(ant_obs)\n",
      "    forward_velocity = np.linalg.norm(torso_vel)  # Magnitude of velocity for locomotion\n",
      "    \n",
      "    # Compute angular velocity for the orientation control task\n",
      "    torso_angular_vel = self.torso_angular_velocity(ant_obs)\n",
      "    angular_velocity_magnitude = np.linalg.norm(torso_angular_vel)  # Magnitude for orientation control\n",
      "    \n",
      "    # Compute goal distance for the directed movement task\n",
      "    goal_distance = self.goal_distance(ant_obs)\n",
      "    \n",
      "    # Reward components\n",
      "    reward_forward = np.tanh(forward_velocity) * velocity_weight  # Reward for forward movement from basic locomotion\n",
      "    reward_orientation_control = -np.tanh(angular_velocity_magnitude) * angular_velocity_weight  # Penalty for high angular velocity, encouraging orientation stability\n",
      "    reward_goal_distance = -np.tanh(goal_distance) * goal_distance_weight  # Higher penalty as the goal distance increases, encouraging movement towards the goal\n",
      "    \n",
      "    # Total reward combines all aspects, factoring in basic locomotion, orientation control, and directed movement towards goal\n",
      "    reward = reward_forward + reward_orientation_control + reward_goal_distance\n",
      "    \n",
      "    # Reward breakdown for potential debugging and analysis\n",
      "    reward_dict = {\n",
      "        'reward_forward': reward_forward,\n",
      "        'reward_orientation_control': reward_orientation_control,\n",
      "        'reward_goal_distance': reward_goal_distance,\n",
      "    }\n",
      "\n",
      "    return reward, reward_dict\n"
     ]
    }
   ],
   "source": [
    "code = api.generate_rewards(2, code_list)\n",
    "code_list.append(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
